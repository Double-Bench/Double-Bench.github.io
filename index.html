<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Seeking and Updating with Live Visual Knowledge">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Seeking and Updating with Live Visual Knowledge</title>
  <script type="module" src=""></script>
 

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./img/gui-logo.jpg">

  <link rel="stylesheet" href="./stylesheets/layout.css">
  <link rel="stylesheet" href="./stylesheets/index.css">
  <link rel="stylesheet" href="./bowe_componets/css/bootstrap.table.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link href="./static/css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="./static/css/custom.css" media="screen" rel="stylesheet" type="text/css" />
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <!-- <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://dongping-chen.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://trustllmbenchmark.github.io/TrustLLM-Website/">
              TrustLLM
            </a>
            <a class="navbar-item" href="https://mllm-judge.github.io">
              MLLM-as-a-Judge
            </a>
            <a class="navbar-item" href="https://unigen-framework.github.io/">
              UniGen
            </a>
            <a class="navbar-item" href="https://github.com/Flossiee/HonestyLLM">
              HonestyLLM
            </a>
            <a class="navbar-item" href="https://llm-coauthor.github.io/">
              LLM-as-a-Coauthor
            </a>
          </div>
        </div>
      </div>

    </div> -->
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Seeking and Updating with Live Visual Knowledge
            </h1>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://dongping-chen.github.io/">Dongping
                  Chen</a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup><sup style="color:#c9892e;">†</sup>,</span>
              <span class="author-block"><a href="https://howiehwong.github.io/">Yue Huang</a><sup style="color:#2bff32;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://siyuan-5.github.io/">Siyuan Wu<a></a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://jingyuhhh.github.io/">Jingyu Tang</a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block">Liuyi Chen<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yilin Bai<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Zhigang He<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Chenlong Wang<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://huichizhou.github.io/">Huichi Zhou</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yiqiang Li<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Tianshuo Zhou<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yue Yu<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://flossiee.github.io/">Chujie Gao</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://mask-hui.github.io/">Qihui Zhang</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yi Gui<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Zhen Li<sup style="color:#ec8bfd;">1</sup>,</span>

              <span class="author-block"><a href="http://wanyao.me/"><b>Yao Wan</b></a><sup style="color:#ec8bfd;">1</sup><sup style="color:#c9892e;">†</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=cTpFPJgAAAAJ&hl=en"><b>Pan
                  Zhou</b></a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=CQ1cqKkAAAAJ&hl=en"><b>Jianfeng
                  Gao</b></a><sup style="color:#fabb55;">3</sup>,</span>
              <span class="author-block"><a href="https://lichao-sun.github.io/"><b>Lichao Sun</b></a><sup style="color:#66f1fb;">4</sup></span>
            </div> -->
            <!-- fmy -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="">Mingyang Fu</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#ff2b67;">*</sup>,
              </span>
              <span class="author-block">
                <a href="">Yuyang Peng</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#ff2b67;">*</sup>,
              </span>
              <span class="author-block">
                <a href="https://dongping-chen.github.io/">Dongping Chen</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#2bff32;">2</sup>
                <sup style="color:#c9892e;">‡</sup>,
              </span>
              <span class="author-block">
                <a href="">Zetong Zhou</a>
                <sup style="color:#ec8bfd;">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://liubl1217.github.io/">Benlin Liu</a>
                <sup style="color:#2bff32;">2</sup>,
              </span>
              <span class="author-block">
                <a href="http://wanyao.me/">Yao Wan</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#c9892e;">†</sup>,
              </span>
              <span class="author-block">
                <a href="https://person.zju.edu.cn/zhaozhou">Zhou Zhao</a>
                <sup style="color:#fabb55;">3</sup>,
              </span>
              <span class="author-block">
                <a href="https://cs.uic.edu/profiles/philip-yu/">Philips S. Yu</a>
                <sup style="color:#66f1fb;">4</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ranjaykrishna.com/index.html">Ranjay Krishna</a>
                <sup style="color:#2bff32;">2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup style="color:#ec8bfd;">1</sup> Huazhong University of Science and Technology
              </span><br>
              <span class="author-block">
                <sup style="color:#2bff32;">2</sup> University of Washington
              </span><br>
              <span class="author-block">
                <sup style="color:#fabb55;">3</sup> Zhejiang University
              </span><br>
              <span class="author-block">
                <sup style="color:#66f1fb;">4</sup> University of Illinois, Chicago
              </span><br>
              <span class="author-block">
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                {<a href="mailto:dongpingchen0612@gmail.com">dongpingchen0612</a>, 
                <a href="mailto:yaowan1992@gmail.com">yaowan1992</a>}@gmail.com, 
                <a href="mailto:ranjay@cs.washington.edu">ranjay@cs.washington.edu</a>
              </span>
            </div>



            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!--                <span class="link-block">-->
                <!--                  <a href="https://arxiv.org/pdf/2011.12948" class="external-link button is-normal is-rounded is-dark">-->
                <!--                    <span class="icon">-->
                <!--                      <i class="fas fa-file-pdf"></i>-->
                <!--                    </span>-->
                <!--                    <span>Paper</span>-->
                <!--                  </a>-->
                <!--                </span>-->
                <span class="link-block">
                  <!-- Arxiv link -->
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!--                <span class="link-block">-->
                <!--                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
                <!--                    class="external-link button is-normal is-rounded is-dark">-->
                <!--                    <span class="icon">-->
                <!--                      <i class="fab fa-youtube"></i>-->
                <!--                    </span>-->
                <!--                    <span>Video</span>-->
                <!--                  </a>-->
                <!--                </span>-->
                <!-- Code Link. -->

                <!-- github links -->
                <span class="link-block"> 
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/shuaishuaicdp/LiveVQA/tree/main"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Model Viewer. -->
                <!-- <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-desktop"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span> -->
                <!-- Slides Link. -->
                <!-- <span class="link-block">
                  <a href="https://docs.google.com/presentation/d/1-r889Nb9n7SeZqrj-ryNqJLoMzp7aGNU2ihO8nUdEcE/edit?usp=sharing"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-google"></i>
                    </span>
                    <span>Slides</span>
                  </a>
                </span> -->
                <!-- Twitter Link. -->
                <!-- <span class="link-block">
                  <a href="https://twitter.com/TianbaoX/status/1778781521253667267" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-twitter"></i>
                    </span>
                    <span>Twitter</span>
                  </a>
                </span> -->
                <!-- Discord Link. -->
                <!-- <span class="link-block">
                  <a href="https://discord.gg/4Gnw7eTEZR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-discord"></i>
                    </span>
                    <span>Discord</span>
                  </a>
                </span> -->

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="column is-full">
        <video controls muted loop autoplay width="100%">
          <source src="static/videos/main.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Main Figure. -->
      <h2 class="title is-3"></h2>
      <div class="content has-text-justified">
        <img src="img/overview.png" width="100%" alt="GUI-world Overview" class="responsive-image">
        <!-- <img src="img/radar.jpg" width="100%" alt="GUI-world Benchmark Overview" class="responsive-image"> -->
        <md-block>
          In this work, we introduce LIVEVQA, a new dataset and benchmark for evaluating Multimodal Large Language Models (MLLMs) on their ability to understand and reason about up-to-date visual information. Specifically, there are three-fold major contributions:
          <ol>
            <li><b>A Novel Dataset.</b> We introduce LIVEVQA, the first dataset of its kind, featuring 107,143 samples across 12 categories, specifically designed to test how MLLMs handle visual information beyond their training data cutoff and how they can be updated with new knowledge.</li>
            <li><b>Comprehensive Benchmarking and Analysis.</b> We conducted extensive benchmarking of 17 state-of-the-art MLLMs, revealing significant performance gaps on content beyond their knowledge cutoff. Our findings show that tool-use or agentic visual seeking frameworks can drastically improve performance by an average of 327%.</li>
            <li><b>Efficient Knowledge Updating Insights.</b> We explored parameter-efficient fine-tuning (PEFT) methods, demonstrating that MLLMs can be efficiently updated with new visual knowledge within a single epoch. While this can impact visual perception, it can enhance knowledge-intensive capabilities, and we provide insights into balancing adapter capacity and model capability.</li>
          </ol>
        </md-block>
      </div>
      <!--/ Main Figure. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-wdith">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <md-block>
              The visual world around us constantly evolves, from real-time news and social media trends to global infrastructure changes visible through satellite imagery and augmented reality enhancements. However, Multimodal Large Language Models (MLLMs), which automate many tasks, struggle to stay current, limited by the cutoff dates in their fixed training datasets.
              To quantify this stagnation, we introduce LiveVQA, the first-of-its-kind dataset featuring 107,143 samples and 12 categories data specifically designed to support research in both seeking and updating with live visual knowledge.
              Drawing from recent news articles, video platforms, and academic publications in April 2024-May 2025, LiveVQA enables evaluation of how models handle latest visual information beyond their knowledge boundaries and how current methods help to update them. 
              Our comprehensive benchmarking of 17 state-of-the-art MLLMs reveals significant performance gaps on content beyond knowledge cutoff, and tool-use or agentic visual seeking framework drastically gain an average of 327% improvement. 
              Furthermore, we explore parameter-efficient fine-tuning (PEFT) methods to update MLLMs with new visual knowledge.
              We dive deeply to the critical balance between adapter capacity and model capability when updating MLLMs with new visual knowledge. 
            </md-block>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">LIVEVQA Dataset Construction</h2>
      <div class="content has-text-justified">
        <img src="img/builder.png" width="100%" alt="environment infrastructure" class="responsive-image">
        <md-block>
          We introduce LIVEVQA, a first-of-its-kind dataset containing fresh visual content and corresponding question-answer pairs, aimed at benchmarking and advancing Multimodal Large Language Models (MLLMs) in seeking and updating live visual knowledge. The visual content is sourced from recent international news articles, YouTube videos, and academic papers spanning from April 2024 to early May 2025.
          The construction of the LIVEVQA dataset primarily follows a multi-stage LLM/MLLM-in-the-loop pipeline with rigorous filtering and human validation:
          <ol>
            <li><b>Raw Data Collection from Diverse Sources</b>: This stage involves collecting recent visual and textual data. For news articles, this includes URL and headline filtering, image selection based on size and relevance (enhanced using GPT-4.1 to ensure strong correlation with events), and semantic deduplication. For videos (from YouTube), it involves preprocessing (restricting to English, max 10 mins, with subtitles), subtitle-based segmentation using an LLM, initial keyframe identification (using UVD and perceptual hashing for deduplication), and LLM-driven selection of top-K relevant keyframes. For academic papers (from arXiv), it includes extracting titles, abstracts, authors, images, and captions, followed by key image selection prioritizing architectural diagrams and key findings, avoiding common visualizations.</li>
            <li><b>Visual Question Answering (VQA) Generation and Filtering</b>: This stage constructs two levels of questions. Level 1 questions target basic visual entity recognition (e.g., locations, persons, time) based on filtered images and metadata, with GPT-4.1 used to filter out unqualified QAs (e.g., those with overly brief answers or simple labels). Level 2 questions are more complex, requiring multi-hop cross-modal reasoning using the full image context and related textual information, covering seven types (location, person, organization, time, event, count, reason); these are also generated and filtered by GPT-4.1 to ensure answer verifiability. All LLM/MLLM-assisted processes undergo human validation with a high pass rate.</li>
          </ol>
        </md-block>

        <div style="margin-top: 30px; text-align: center;">
          <h3 class="title is-4">Data Filtering</h3>
          <img src="img/filter.png" width="100%" alt="LIVEVQA Filtering Process" class="responsive-image" style="max-width: 800px; margin-bottom: 10px; border: 1px solid #ddd;">
          <md-block>
            <p style="font-size: 0.9em; color: #363636; text-align: justify;">
            The diagram above illustrates the comprehensive filtering process employed in the construction of the LIVEVQA dataset. It details how raw images and synthesized question-answer pairs are systematically refined across three distinct data source pipelines: YouTube videos, arXiv academic papers, and news articles (from sources such as Forbes, Variety, CNN, BBC, and Associated Press).
            The pipeline begins with a large corpus of "Raw Images" (e.g., 829K from YouTube, 180K from arXiv, 19K from News). These are then subjected to a series of stringent filtering stages. Key steps include "Key Frame Filters" for video content, "Irrelevant Image Filters" to remove non-pertinent visuals, and "Choose the Most Representative" to select the most informative images. Further refinement occurs through "Level-1 QAs Filters" and "Level-2 QAs Filters", followed by an "AI Judge & Filter QAs" step. This meticulous process significantly reduces the volume of data, ensuring that only high-quality and relevant "Meta Images" and their associated reasoning questions (e.g., culminating in 12K images from YouTube, 9K from arXiv, and 8K from News) are included in the final LIVEVQA dataset. This multi-layered filtering strategy is essential for maintaining the integrity and utility of the benchmark.
            </p>
          </md-block>
        </div>

      </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">LIVEVQA Dataset Statistics</h2>
      <md-block>
        Below we present an overview of the main statistics of LIVEVQA, showcasing its composition across different data sources and splits. LIVEVQA contains a total of 28,488 unique images and 107,143 questions.
      </md-block>
      <div class="column is-full-width interpolation-panel">
        <div class="table-container" style="margin-bottom: 20px;"> <table class="table is-hoverable is-striped is-bordered" style="margin: 0 auto; background-color: rgba(0,0,0,0); width: 100%; text-align: center;">
            <thead>
              <tr>
                <th style="text-align: left;">Category</th>
                <th>Images</th>
                <th>#Question</th>
                <th>Level 1</th>
                <th>Level 2</th>
                <th>Avg. Len.</th>
                <th>Purpose</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="font-weight: bold; text-align: left;">News Article</td>
                <td>7,579</td>
                <td>38,809</td>
                <td>7,579</td>
                <td>31,230</td>
                <td>749</td>
                <td>-</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">YouTube Videos</td>
                <td>11,948</td>
                <td>43,168</td>
                <td>11,948</td>
                <td>31,220</td>
                <td>311</td>
                <td>-</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">Academic Paper</td>
                <td>8,961</td>
                <td>25,166</td>
                <td>9,456</td>
                <td>16,205</td>
                <td>597</td>
                <td>-</td>
              </tr>
              <tr>
                <td colspan="7" style="border-top: 2px solid #dbdbdb;"></td> </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">Avg. per Sample</td>
                <td>1</td>
                <td>3.86</td>
                <td>1</td>
                <td>2.86</td>
                <td>517</td>
                <td>-</td>
              </tr>
              <tr>
                <td colspan="7" style="border-top: 2px solid #dbdbdb;"></td> </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">Test Split</td>
                <td>1,500</td>
                <td>3,000</td>
                <td>1,500</td>
                <td>1,500</td>
                <td>544</td>
                <td>Exp. 1</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">Training Split</td>
                <td>26,988</td>
                <td>104,143</td>
                <td>26,988</td>
                <td>77,150</td>
                <td>496</td>
                <td>Exp. 2</td>
              </tr>
            </tbody>
          </table>
        </div>

        <div style="text-align: center;">
            <img src="img/static.png" alt="Dataset Statistics Distributions" style="width: 100%; max-width: 700px; margin-top: 10px; display: block; margin-left: auto; margin-right: auto;"> <p style="font-size: 0.9em; color: #555; margin-top: 5px; text-align: center;">Figure 4: (Left) Image size distribution in YouTube image filtering pipeline. (Right) Textual context length distribution for each question.</p>
        </div>
      </div>
      </div>
  </section>

  <section>
    <div class="container is-max-desktop">

      <h2 class="title is-3">Benchmark Results for LIVEVQA</h2>
      <md-block>
        We conducted a comprehensive benchmark of 17 state-of-the-art Multimodal Large Language Models (MLLMs) to evaluate their capabilities in seeking and updating live visual knowledge. The evaluation was performed on the LIVEVQA dataset, which includes content from recent news articles, YouTube videos, and academic papers. Performance was measured for Level 1 (visual entity recognition) and Level 2 (deeper visual knowledge reasoning) questions, with and without various search augmentation methods.
        Key findings indicate that current MLLMs struggle significantly with visual knowledge beyond their training cutoff, but performance is drastically improved with the use of multimodal search tools.
      </md-block>
    </div>
    <div class="cover" id="contentCover">
      <div class="container-t">
        <div class="row">
          <div class="col-md-12">
            <div class="infoCard">
              <div class="infoBody">
                <div class="tabs is-centered example_lst">
                  <ul>
                    <li class="is-active"><a title="Overall Accuracy">Overall Accuracy</a></li>
                    <li><a title="News Subset Details">News Subset - Detailed Categories</a></li>
                    <li><a title="Video Subset Details">Video Subset - Detailed Categories</a></li>
                  </ul>
                </div>
                <script type="text/javascript">
                  document.querySelectorAll(".example_lst li").forEach(e => {
                    e.addEventListener("click", Click_1)
                  })

                  function Click_1(eve) {
                    const iTxt = eve.target.closest('a').title; // Use title attribute for reliable selection
                    document.querySelectorAll(".example_lst li").forEach(v_li => {
                      if (iTxt === v_li.querySelector('a').title) {
                        v_li.classList.add("is-active");
                      } else {
                        v_li.classList.remove("is-active");
                      }
                    });
                    document.querySelectorAll(".lib_examples").forEach(block => {
                      block.style.display = (block.title === iTxt) ? 'block' : 'none';
                    });
                  }
                  // Initialize first tab
                  document.addEventListener('DOMContentLoaded', () => {
                    const firstTabLink = document.querySelector(".example_lst li.is-active a");
                    if (firstTabLink) {
                        Click_1({ target: firstTabLink });
                    }
                  });
                </script>

                <div title="Overall Accuracy" class="lib_examples" id="BoardPanel1" style="display: block;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Accuracy (%) of visual factuality seeking benchmark in open-ended format. (Avg. = Average)
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size: 0.85em;">
                      <thead>
                        <tr>
                          <th rowspan="2">Model</th>
                          <th rowspan="2">Cutoff</th>
                          <th colspan="4">Level 1</th>
                          <th colspan="4">Level 2</th>
                        </tr>
                        <tr>
                          <th>News</th>
                          <th>Video</th>
                          <th>Arxiv</th>
                          <th>Avg.</th>
                          <th>News</th>
                          <th>Video</th>
                          <th>Arxiv</th>
                          <th>Avg.</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="10" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w.o. Search</td></tr>
                        <tr><td>GPT-4.1</td><td>Jun. 2024</td><td>27.0</td><td>22.0</td><td>0.4</td><td><strong>16.5</strong></td><td>5.2</td><td>7.2</td><td>0.2</td><td>3.0</td></tr>
                        <tr><td>GPT-4.1-mini</td><td>Jun. 2024</td><td>24.6</td><td>19.6</td><td>0.2</td><td>14.8</td><td>4.0</td><td>7.8</td><td>0.4</td><td>4.0</td></tr>
                        <tr><td>GPT-4.1-nano</td><td>Jun. 2024</td><td>13.0</td><td>13.0</td><td>0.0</td><td>8.6</td><td>2.2</td><td>6.0</td><td>0.4</td><td>2.9</td></tr>
                        <tr><td>Gemini-2.5-Flash</td><td>Jan. 2025</td><td>25.8</td><td>18.4</td><td>0.8</td><td>15.0</td><td>4.6</td><td>4.4</td><td>4.0</td><td><strong>4.3</strong></td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>Jan. 2025</td><td>28.0</td><td>17.4</td><td>0.6</td><td><strong>15.3</strong></td><td>4.4</td><td>2.4</td><td>1.2</td><td>2.7</td></tr>
                        <tr><td>Gemma-3-27B-It</td><td>Aug. 2024</td><td>21.0</td><td>16.4</td><td>1.0</td><td>12.8</td><td>3.8</td><td>4.6</td><td>6.2</td><td><strong>4.9</strong></td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>Oct. 2024</td><td>26.2</td><td>16.4</td><td>0.6</td><td>14.3</td><td>2.2</td><td>4.4</td><td>4.4</td><td>3.7</td></tr>
                        <tr><td>Qwen-2.5-VL-7B-Instruct</td><td>Unknown</td><td>20.2</td><td>13.4</td><td>0.2</td><td>11.3</td><td>3.8</td><td>5.4</td><td>2.0</td><td>3.7</td></tr>
                        <tr><td>Qwen-2.5-VL-32B-Instruct</td><td>Unknown</td><td>25.2</td><td>16.4</td><td>0.4</td><td>14.0</td><td>4.2</td><td>5.6</td><td>1.2</td><td>3.7</td></tr>
                        <tr><td>Qwen-2.5-VL-72B-Instruct</td><td>Unknown</td><td>12.4</td><td>9.4</td><td>0.0</td><td>7.3</td><td>1.4</td><td>3.6</td><td>3.6</td><td>2.9</td></tr>
                        <tr><td>Llama-4-Scout</td><td>Aug. 2024</td><td>20.6</td><td>16.4</td><td>0.0</td><td>12.1</td><td>4.0</td><td>5.0</td><td>2.8</td><td>3.9</td></tr>
                        <tr><td>Llama-4-Maverick</td><td>Aug. 2024</td><td>20.2</td><td>19.0</td><td>0.6</td><td><strong>13.3</strong></td><td>5.8</td><td>6.0</td><td>5.2</td><td><strong>5.7</strong></td></tr>
                        <tr><td colspan="10" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Text Search</td></tr>
                        <tr><td>GPT-4.1</td><td>Jun. 2024</td><td>25.0</td><td>21.4</td><td>0.6</td><td><strong>15.6</strong></td><td>3.6</td><td>5.6</td><td>3.8</td><td><strong>4.3</strong></td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>Jan. 2025</td><td>17.6</td><td>9.2</td><td>0.2</td><td>9.0</td><td>2.0</td><td>1.6</td><td>1.0</td><td>1.5</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>Oct. 2024</td><td>24.6</td><td>16.6</td><td>0.0</td><td>13.7</td><td>2.0</td><td>3.6</td><td>4.8</td><td>3.5</td></tr>
                        <tr><td colspan="10" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Native Image Search</td></tr>
                        <tr><td>GPT-03</td><td>Jun. 2024</td><td><strong>33.6</strong></td><td><strong>33.6</strong></td><td>2.6</td><td><strong>23.3</strong></td><td><strong>14.6</strong></td><td><strong>14.9</strong></td><td><strong>17.8</strong></td><td><strong>15.8</strong></td></tr>
                        <tr><td colspan="10" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. MM-Search [Jiang et al., 2024]</td></tr>
                        <tr><td>GPT-4.1</td><td>Jun. 2024</td><td><strong>42.0</strong></td><td><strong>36.1</strong></td><td><strong>22.0</strong></td><td><strong>33.4</strong></td><td><strong>27.2</strong></td><td><strong>15.2</strong></td><td><strong>48.8</strong></td><td><strong>30.4</strong></td></tr>
                      </tbody>
                    </table>
                  </div>
                </div>

                <div title="News Subset Details" class="lib_examples" id="BoardPanel2" style="display: none;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Comparison performance on detailed categories in News subset. (Loc.=Location, Per.=Person, Org.=Organization, Eve.=Event, Obj.=Object, Cou.=Count, Rea.=Reason, Avg.=Average)
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size:0.8em;">
                      <thead>
                        <tr>
                          <th rowspan="2">Model</th>
                          <th colspan="6">Level 1 (News Subset)</th>
                          <th colspan="8">Level 2 (News Subset)</th>
                        </tr>
                        <tr>
                          <th>Loc.</th><th>Per.</th><th>Org.</th><th>Eve.</th><th>Obj.</th><th>Avg.</th>
                          <th>Loc.</th><th>Per.</th><th>Org.</th><th>Time</th><th>Cou.</th><th>Rea.</th><th>Eve.</th><th>Avg.</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w.o. Search</td></tr>
                        <tr><td>GPT-4.1</td><td><strong>50.72</strong></td><td>15.19</td><td>35.89</td><td>27.03</td><td>6.28</td><td>28.81</td><td>0.00</td><td>1.75</td><td><strong>11.68</strong></td><td>3.82</td><td>7.84</td><td>1.63</td><td>0.00</td><td>5.05</td></tr>
                        <tr><td>GPT-4.1-mini</td><td>33.33</td><td>10.91</td><td>45.59</td><td>11.86</td><td>19.23</td><td>24.60</td><td>0.00</td><td>3.57</td><td>8.82</td><td>0.00</td><td>10.24</td><td>0.00</td><td>0.00</td><td>4.00</td></tr>
                        <tr><td>GPT-4.1-Nano</td><td>16.16</td><td>3.64</td><td>30.88</td><td>3.39</td><td>13.00</td><td>13.00</td><td>0.00</td><td>0.00</td><td>4.41</td><td>1.54</td><td>3.94</td><td>0.83</td><td>0.00</td><td>2.20</td></tr>
                        <tr><td>Gemini-2.5-Flash</td><td>26.26</td><td>37.27</td><td>35.29</td><td>7.63</td><td>25.80</td><td>25.80</td><td>0.00</td><td>3.57</td><td>1.47</td><td>3.85</td><td>8.66</td><td>4.17</td><td>0.00</td><td>4.60</td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>23.23</td><td><strong>46.36</strong></td><td>35.29</td><td>10.17</td><td>28.00</td><td>28.00</td><td>3.57</td><td>0.00</td><td>5.88</td><td>3.08</td><td>3.94</td><td>6.67</td><td>0.00</td><td>4.40</td></tr>
                        <tr><td>Gemma-3-27B-IT</td><td>24.24</td><td>15.45</td><td>38.24</td><td>8.47</td><td>21.00</td><td>21.00</td><td>3.57</td><td>0.00</td><td>8.82</td><td>1.54</td><td>7.87</td><td>0.00</td><td>0.00</td><td>3.80</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>26.20</td><td>38.38</td><td>10.00</td><td>14.41</td><td>26.20</td><td>26.20</td><td>0.00</td><td>0.00</td><td>4.41</td><td>2.31</td><td>1.57</td><td>2.50</td><td>0.00</td><td>2.20</td></tr>
                        <tr><td>Qwen-2.5-VL-7B</td><td>23.23</td><td>21.15</td><td>30.88</td><td>12.71</td><td>20.20</td><td>20.20</td><td>0.00</td><td>0.00</td><td>4.41</td><td>1.54</td><td>7.09</td><td>4.17</td><td>0.00</td><td>3.80</td></tr>
                        <tr><td>Qwen-2.5-VL-32B</td><td>33.33</td><td>18.18</td><td>30.88</td><td>18.64</td><td>25.20</td><td>25.20</td><td>0.00</td><td>0.00</td><td>7.35</td><td>2.31</td><td>6.30</td><td>4.17</td><td>0.00</td><td>4.20</td></tr>
                        <tr><td>Qwen-2.5-VL-72B</td><td>12.50</td><td>6.36</td><td>15.15</td><td>8.47</td><td>12.40</td><td>12.40</td><td>0.00</td><td>0.00</td><td>4.41</td><td>0.77</td><td>1.57</td><td>0.83</td><td>0.00</td><td>1.40</td></tr>
                        <tr><td>Llama-4-Scout</td><td>26.26</td><td>13.64</td><td>35.29</td><td>8.47</td><td>20.60</td><td>20.60</td><td>3.57</td><td>0.00</td><td>4.41</td><td>3.08</td><td>9.45</td><td>0.00</td><td>0.00</td><td>4.00</td></tr>
                        <tr><td>Llama-4-Maverick</td><td>20.20</td><td>19.09</td><td>36.76</td><td>5.93</td><td>20.20</td><td>20.20</td><td>0.00</td><td>0.00</td><td><strong>10.29</strong></td><td>2.31</td><td><strong>13.39</strong></td><td>1.67</td><td>0.00</td><td><strong>5.80</strong></td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Text Search</td></tr>
                        <tr><td>GPT-4.1</td><td>34.62</td><td>13.56</td><td>48.53</td><td>2.73</td><td>25.00</td><td>25.00</td><td>5.88</td><td>3.57</td><td>5.88</td><td>3.85</td><td>4.72</td><td>0.83</td><td>0.00</td><td>3.60</td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>18.18</td><td>10.17</td><td>29.41</td><td>12.73</td><td>17.60</td><td>17.60</td><td>0.00</td><td>3.57</td><td>4.41</td><td>1.54</td><td>2.36</td><td>1.67</td><td>0.00</td><td>2.00</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>23.08</td><td>18.64</td><td>40.38</td><td>6.36</td><td>24.60</td><td>24.60</td><td>0.00</td><td>5.88</td><td>1.47</td><td>1.54</td><td>3.15</td><td>0.83</td><td>0.00</td><td>2.00</td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Native Image Search</td></tr>
                        <tr><td>GPT-03</td><td><strong>47.47</strong></td><td>23.73</td><td><strong>57.35</strong></td><td><strong>47.12</strong></td><td>33.60</td><td><strong>33.60</strong></td><td>0.00</td><td>17.86</td><td><strong>20.59</strong></td><td>7.69</td><td><strong>17.32</strong></td><td><strong>17.50</strong></td><td><strong>10.00</strong></td><td><strong>14.60</strong></td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. MM-Search [Jiang et al., 2024]</td></tr>
                        <tr><td>GPT-4.1</td><td><strong>50.00</strong></td><td>35.78</td><td><strong>55.88</strong></td><td><strong>42.86</strong></td><td>42.00</td><td><strong>42.00</strong></td><td><strong>15.50</strong></td><td><strong>23.53</strong></td><td><strong>30.88</strong></td><td><strong>42.52</strong></td><td><strong>20.00</strong></td><td><strong>46.43</strong></td><td>0.00</td><td><strong>27.20</strong></td></tr>
                      </tbody>
                    </table>
                  </div>
                </div>

                 <div title="Video Subset Details" class="lib_examples" id="BoardPanel3" style="display: none;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Performance on detailed categories in Video subset. (Loc.=Location, Per.=Person, Org.=Organization, Eve.=Event, Obj.=Object, Cou.=Count, Rea.=Reason, Avg.=Average)
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size:0.8em;">
                      <thead>
                        <tr>
                          <th rowspan="2">Model</th>
                          <th colspan="6">Level 1 (Video Subset)</th>
                          <th colspan="8">Level 2 (Video Subset)</th>
                        </tr>
                        <tr>
                          <th>Loc.</th><th>Per.</th><th>Org.</th><th>Eve.</th><th>Obj.</th><th>Avg.</th>
                          <th>Loc.</th><th>Per.</th><th>Org.</th><th>Time</th><th>Cou.</th><th>Rea.</th><th>Eve.</th><th>Avg.</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w.o. Search</td></tr>
                        <tr><td>GPT-4.1</td><td>26.58</td><td>8.33</td><td><strong>40.85</strong></td><td>7.77</td><td><strong>32.23</strong></td><td><strong>22.00</strong></td><td><strong>8.51</strong></td><td>3.45</td><td>5.56</td><td>6.32</td><td><strong>11.20</strong></td><td>5.65</td><td>4.55</td><td><strong>7.20</strong></td></tr>
                        <tr><td>GPT-4.1-mini</td><td>21.52</td><td>13.54</td><td>30.99</td><td>4.85</td><td><strong>30.58</strong></td><td>19.60</td><td>2.13</td><td>3.45</td><td><strong>12.96</strong></td><td>6.32</td><td><strong>15.20</strong></td><td>3.23</td><td>4.55</td><td><strong>7.80</strong></td></tr>
                        <tr><td>GPT-4.1-nano</td><td>15.19</td><td>1.04</td><td>28.17</td><td>4.85</td><td>19.01</td><td>13.00</td><td>0.00</td><td>0.00</td><td>5.56</td><td>6.32</td><td>14.40</td><td>2.42</td><td>0.00</td><td>6.00</td></tr>
                        <tr><td>Gemini-2.5-Flash</td><td>18.99</td><td>27.08</td><td>29.58</td><td>4.85</td><td>18.18</td><td>18.40</td><td>0.00</td><td>3.45</td><td>1.85</td><td>4.21</td><td>11.20</td><td>0.81</td><td>4.55</td><td>4.40</td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>8.86</td><td>25.00</td><td>32.39</td><td>6.80</td><td>19.01</td><td>17.40</td><td>0.00</td><td>0.00</td><td>1.85</td><td>2.11</td><td>5.60</td><td>1.61</td><td>0.00</td><td>2.40</td></tr>
                        <tr><td>Gemma-3-27B-IT</td><td>13.92</td><td>14.58</td><td>33.80</td><td>3.88</td><td>21.49</td><td>16.40</td><td>0.00</td><td>0.00</td><td>5.56</td><td>4.21</td><td>10.40</td><td>1.61</td><td>4.55</td><td>4.60</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>18.99</td><td>7.29</td><td>29.58</td><td>6.80</td><td>23.97</td><td>16.40</td><td>2.13</td><td>0.00</td><td>1.85</td><td>4.21</td><td>7.20</td><td>4.84</td><td>4.55</td><td>4.40</td></tr>
                        <tr><td>Qwen-2.5-VL-7B</td><td>12.66</td><td>10.42</td><td>25.35</td><td>4.85</td><td>16.53</td><td>13.40</td><td>2.13</td><td>0.00</td><td>5.56</td><td>3.16</td><td>14.40</td><td>1.61</td><td>0.00</td><td>5.40</td></tr>
                        <tr><td>Qwen-2.5-VL-32B</td><td>16.46</td><td>10.42</td><td>32.39</td><td>4.85</td><td>22.31</td><td>16.40</td><td>0.00</td><td>0.00</td><td>5.56</td><td>6.32</td><td>9.60</td><td>4.84</td><td>4.55</td><td>5.60</td></tr>
                        <tr><td>Qwen-2.5-VL-72B</td><td>10.13</td><td>3.12</td><td>18.31</td><td>1.94</td><td>14.88</td><td>9.40</td><td>0.00</td><td>0.00</td><td>7.41</td><td>3.16</td><td>5.60</td><td>2.42</td><td>4.55</td><td>3.60</td></tr>
                        <tr><td>Llama-4-Scout</td><td>16.46</td><td>13.54</td><td>26.76</td><td>7.77</td><td>20.66</td><td>16.40</td><td>2.13</td><td>0.00</td><td>7.41</td><td>4.21</td><td>10.40</td><td>1.61</td><td>4.55</td><td>5.00</td></tr>
                        <tr><td>Llama-4-Maverick</td><td>18.99</td><td>14.58</td><td>38.03</td><td>8.74</td><td>20.66</td><td>19.00</td><td>2.13</td><td>3.45</td><td>3.70</td><td>4.21</td><td>15.20</td><td>2.42</td><td>0.00</td><td>6.00</td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Text Search</td></tr>
                        <tr><td>GPT-4.1</td><td>13.92</td><td>6.25</td><td>30.05</td><td>3.56</td><td>22.59</td><td>14.60</td><td>2.84</td><td>0.00</td><td>3.09</td><td>3.86</td><td>6.67</td><td>2.42</td><td>3.03</td><td>3.73</td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>1.69</td><td>1.39</td><td>19.72</td><td>2.91</td><td>8.54</td><td>6.53</td><td>0.00</td><td>0.00</td><td>0.62</td><td>1.40</td><td>3.20</td><td>0.00</td><td>1.52</td><td>1.20</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>8.02</td><td>4.17</td><td>14.55</td><td>2.59</td><td>12.95</td><td>8.33</td><td>1.42</td><td>0.00</td><td>1.23</td><td>1.40</td><td>3.73</td><td>0.54</td><td>0.00</td><td>1.60</td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Native Image Search</td></tr>
                        <tr><td>GPT-o3</td><td><strong>37.97</strong></td><td>19.79</td><td><strong>43.66</strong></td><td><strong>22.33</strong></td><td><strong>46.28</strong></td><td><strong>33.60</strong></td><td>8.51</td><td><strong>10.34</strong></td><td><strong>12.96</strong></td><td><strong>11.58</strong></td><td><strong>29.60</strong></td><td><strong>25.00</strong></td><td><strong>18.18</strong></td><td><strong>19.40</strong></td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. MM-Search [Jiang et al., 2024]</td></tr>
                        <tr><td>GPT-4.1</td><td><strong>29.11</strong></td><td><strong>31.58</strong></td><td><strong>49.30</strong></td><td><strong>21.36</strong></td><td><strong>38.84</strong></td><td><strong>33.00</strong></td><td><strong>13.68</strong></td><td><strong>17.02</strong></td><td>10.34</td><td>11.11</td><td><strong>26.40</strong></td><td>9.68</td><td>4.55</td><td><strong>15.20</strong></td></tr>
                      </tbody>
                    </table>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="Empirical Result">
    <div class="container is-max-desktop">
      <div class="featurecard-container">
        <h1 class="title">Empirical Results from LIVEVQA</h1>

        <div class="feature_1x1">
          <div class="featurecard">
            <h2>MLLMs Face Challenges with "Live" Visual Knowledge; Multimodal Search is Key</h2>
          </div>
          <div class="description">
            <p>
              Our comprehensive benchmarking of 17 state-of-the-art Multimodal Large Language Models (MLLMs) on the LIVEVQA dataset revealed significant difficulties in handling visual information beyond their knowledge cutoff dates. For instance, even top-performing models showed low accuracy on recent visual content when operating without external tools.
            </p>
            <p>
              However, the integration of multimodal search capabilities leads to dramatic improvements.
              <ol>
                <li>Models augmented with multimodal search tools (e.g., GPT-4.1 with MM-Search) demonstrated an average accuracy increase of 327% in seeking live visual knowledge. Specifically, GPT-4.1's average accuracy more than doubled from 16.5% to 33.4% when using MM-Search, with particularly striking gains on challenging Level 2 questions (e.g., accuracy on News subset Level 2 rose from 5.2% to 27.2%).</li>
                <li>Native image search capabilities, as seen in models like GPT-03, also provided substantial gains (e.g., from 3.0% to 15.8% on Level 2 questions). In contrast, simple text-based online searching did not yield significant improvements, underscoring the necessity of multimodal retrieval for dynamic visual information.</li>
              </ol>
            </p>
          </div>
        </div>

        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Efficiently Updating MLLMs with New Visual Knowledge via PEFT</h2>
          </div>
          <div class="description">
            <p>
              The research explored updating MLLMs with new visual knowledge using Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and DoRA.
              <ol>
                <li><b>Rapid Adaptation:</b> Visual information can be efficiently updated through fine-tuning within only one epoch. Models using direct multiple-choice questions with concise answers (MCQA format) yielded faster and more effective learning during the visual knowledge acquisition phase compared to other formats like QA (Question + Ground Truth) or QAR (Question + Ground Truth + Reasoning).</li>
                <li><b>LoRA Rank Impact:</b> Higher rank LoRA configurations consistently enhanced visual knowledge capabilities, particularly in assimilating recent visual entities. Models with higher ranks outperformed lower-rank counterparts by an average of 5.4% on the validation subset.</li>
                <li><b>Benefit to General Reasoning:</b> Training on the visually knowledge-intensive LIVEVQA dataset—particularly with straightforward answers and multiple-choice questions—led to a notable 4.2% improvement on the general multimodal reasoning benchmark MMMU.</li>
              </ol>
            </p>
          </div>
        </div>

        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Knowledge Updating Presents Trade-offs: Enhanced Reasoning vs. Degraded Perception</h2>
          </div>
          <div class="description">
            <p>
              While PEFT methods allow for efficient incorporation of new visual facts, this process is not without its challenges and trade-offs.
            </p>
            <p>
              A consistent observation was the degradation in the model's foundational visual perception capabilities (as measured by the MMStar benchmark) after undergoing intensive visual knowledge updates, regardless of rank, training steps, or data formats. For example, models trained using the simple QA format exhibited a performance drop on MMStar from 65.80% to 58.16%. This suggests an inherent conflict between enhancing specific visual knowledge through intensive updates and preserving the model's broader visual perception abilities.
            </p>
          </div>
        </div>

        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Model Scale Correlates with Performance, but Calibration Remains a Challenge</h2>
          </div>
          <div class="description">
            <p>
              The benchmark results highlighted several aspects regarding model characteristics:
              <ol>
                <li><b>Larger Models Tend to Perform Better:</b> For models sharing the same knowledge cutoff (e.g., the GPT-4.1 family), increased model size generally correlated with improved accuracy on LIVEVQA tasks across all difficulty levels. Proprietary models also typically maintained an advantage over open-source counterparts.</li>
                <li><b>Overconfidence and Calibration Issues:</b> A crucial finding was the positive correlation between stated confidence and accuracy across models, but with significant calibration issues. All evaluated MLLMs demonstrated a consistent pattern of overconfidence in their visual factuality assessments, with their performance falling significantly below the ideal calibration line. While larger models like GPT-4.1 showed comparatively better calibration than their smaller variants, substantial opportunities remain for improving MLLM calibration when encountering unknown visual knowledge.</li>
                <li><b>Level 2 Questions Prove More Difficult:</b> As anticipated, Level 2 questions, which require deeper cross-modal reasoning, generally resulted in significantly lower performance for models compared to Level 1 (visual entity recognition) questions across most data subsets (News, Video).</li>
              </ol>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- <section class="section" id="Acknoledgement">
    <div class="container is-max-desktop content">
      <h1 class="supportTitle">Acknowledgement</h1>
      <md-block>
        Many thanks to Yinuo Liu, Zhengyan Fu, Shilin Zhang, Yu, Tianhe Gu, Haokuan Yuan, and Junqi Wang for their
        invalueble effort in this project.
        This project is based on methodologies and code presented in
        [Videochat2](https://github.com/OpenGVLab/Ask-Anything).
        This website is based on templates in [TrustLLM](https://trustllmbenchmark.github.io/TrustLLM-Website/) and
        [OSWorld](https://os-world.github.io/).
      </md-block>
    </div>
  </section> -->

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h1 class="title">BibTeX</h1>
      <pre><code>@misc{chen2024guiworld,
        title={Seeking and Updating with Live Visual Knowledge}, 
        author={Dongping Chen and Yue Huang and Siyuan Wu and Jingyu Tang and Liuyi Chen and Yilin Bai and Zhigang He and Chenlong Wang and Huichi Zhou and Yiqiang Li and Tianshuo Zhou and Yue Yu and Chujie Gao and Qihui Zhang and Yi Gui and Zhen Li and Yao Wan and Pan Zhou and Jianfeng Gao and Lichao Sun},
        year={2024},
        eprint={2406.10819},
  }</code></pre>
    </div>
  </section> -->


  <div class="content">
    <div id="supportContainer">
      <h1 class="supportTitle">LIVEVQA Team</h1>
      <br>

      <div id="logoContainer">
        <img src="img/logos/HUST.png" alt="School 1" class="schoolLogo">
        <img src="img/logos/UW.png" alt="School 2" class="schoolLogo">
        <img src="img/logos/ZJU.png" alt="School 3" class="schoolLogo">
        <!-- <img src="img/logos/ND.png" alt="School 3" class="schoolLogo"> -->
        <img src="img/logos/uic.png" alt="School 4" class="schoolLogo">

      </div>


    </div>
  </div>
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.

            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>