<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Seeking and Updating with Live Visual Knowledge">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Seeking and Updating with Live Visual Knowledge</title>
  <script type="module" src=""></script>
 

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./img/gui-logo.jpg">

  <link rel="stylesheet" href="./stylesheets/layout.css">
  <link rel="stylesheet" href="./stylesheets/index.css">
  <link rel="stylesheet" href="./bowe_componets/css/bootstrap.table.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link href="./static/css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="./static/css/custom.css" media="screen" rel="stylesheet" type="text/css" />
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <!-- <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://dongping-chen.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://trustllmbenchmark.github.io/TrustLLM-Website/">
              TrustLLM
            </a>
            <a class="navbar-item" href="https://mllm-judge.github.io">
              MLLM-as-a-Judge
            </a>
            <a class="navbar-item" href="https://unigen-framework.github.io/">
              UniGen
            </a>
            <a class="navbar-item" href="https://github.com/Flossiee/HonestyLLM">
              HonestyLLM
            </a>
            <a class="navbar-item" href="https://llm-coauthor.github.io/">
              LLM-as-a-Coauthor
            </a>
          </div>
        </div>
      </div>

    </div> -->
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Seeking and Updating with Live Visual Knowledge
            </h1>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://dongping-chen.github.io/">Dongping
                  Chen</a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup><sup style="color:#c9892e;">†</sup>,</span>
              <span class="author-block"><a href="https://howiehwong.github.io/">Yue Huang</a><sup style="color:#2bff32;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://siyuan-5.github.io/">Siyuan Wu<a></a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://jingyuhhh.github.io/">Jingyu Tang</a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block">Liuyi Chen<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yilin Bai<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Zhigang He<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Chenlong Wang<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://huichizhou.github.io/">Huichi Zhou</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yiqiang Li<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Tianshuo Zhou<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yue Yu<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://flossiee.github.io/">Chujie Gao</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://mask-hui.github.io/">Qihui Zhang</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yi Gui<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Zhen Li<sup style="color:#ec8bfd;">1</sup>,</span>

              <span class="author-block"><a href="http://wanyao.me/"><b>Yao Wan</b></a><sup style="color:#ec8bfd;">1</sup><sup style="color:#c9892e;">†</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=cTpFPJgAAAAJ&hl=en"><b>Pan
                  Zhou</b></a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=CQ1cqKkAAAAJ&hl=en"><b>Jianfeng
                  Gao</b></a><sup style="color:#fabb55;">3</sup>,</span>
              <span class="author-block"><a href="https://lichao-sun.github.io/"><b>Lichao Sun</b></a><sup style="color:#66f1fb;">4</sup></span>
            </div> -->
            <!-- fmy -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="">Mingyang Fu</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#ff2b67;">*</sup>,
              </span>
              <span class="author-block">
                <a href="">Yuyang Peng</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#ff2b67;">*</sup>,
              </span>
              <span class="author-block">
                <a href="">Dongping Chen</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#2bff32;">2</sup>
                <sup style="color:#c9892e;">‡</sup>,
              </span>
              <span class="author-block">
                <a href="">Zetong Zhou</a>
                <sup style="color:#ec8bfd;">1</sup>,
              </span>
              <span class="author-block">
                <a href="">Benlin Liu</a>
                <sup style="color:#2bff32;">2</sup>,
              </span>
              <span class="author-block">
                <a href="">Yao Wan</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#c9892e;">†</sup>,
              </span>
              <span class="author-block">
                <a href="">Zhou Zhao</a>
                <sup style="color:#fabb55;">3</sup>,
              </span>
              <span class="author-block">
                <a href="">Philips S. Yu</a>
                <sup style="color:#66f1fb;">4</sup>,
              </span>
              <span class="author-block">
                <a href="">Ranjay Krishna</a>
                <sup style="color:#2bff32;">2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup style="color:#ec8bfd;">1</sup> Huazhong University of Science and Technology
              </span><br>
              <span class="author-block">
                <sup style="color:#2bff32;">2</sup> University of Washington
              </span><br>
              <span class="author-block">
                <sup style="color:#fabb55;">3</sup> Zhejiang University
              </span><br>
              <span class="author-block">
                <sup style="color:#66f1fb;">4</sup> University of Illinois, Chicago
              </span><br>
              <span class="author-block">
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">{<a href="mailto:dongpingchen0612@gmail.com">dongpingchen0612</a>, <a
                  href="mailto:yaowan1992@gmail.com">yaowan1992</a>}@gmail.com</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!--                <span class="link-block">-->
                <!--                  <a href="https://arxiv.org/pdf/2011.12948" class="external-link button is-normal is-rounded is-dark">-->
                <!--                    <span class="icon">-->
                <!--                      <i class="fas fa-file-pdf"></i>-->
                <!--                    </span>-->
                <!--                    <span>Paper</span>-->
                <!--                  </a>-->
                <!--                </span>-->
                <span class="link-block">
                  <!-- Arxiv link -->
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!--                <span class="link-block">-->
                <!--                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
                <!--                    class="external-link button is-normal is-rounded is-dark">-->
                <!--                    <span class="icon">-->
                <!--                      <i class="fab fa-youtube"></i>-->
                <!--                    </span>-->
                <!--                    <span>Video</span>-->
                <!--                  </a>-->
                <!--                </span>-->
                <!-- Code Link. -->

                <!-- github links -->
                <span class="link-block"> 
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href=""
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Model Viewer. -->
                <!-- <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-desktop"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span> -->
                <!-- Slides Link. -->
                <!-- <span class="link-block">
                  <a href="https://docs.google.com/presentation/d/1-r889Nb9n7SeZqrj-ryNqJLoMzp7aGNU2ihO8nUdEcE/edit?usp=sharing"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-google"></i>
                    </span>
                    <span>Slides</span>
                  </a>
                </span> -->
                <!-- Twitter Link. -->
                <!-- <span class="link-block">
                  <a href="https://twitter.com/TianbaoX/status/1778781521253667267" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-twitter"></i>
                    </span>
                    <span>Twitter</span>
                  </a>
                </span> -->
                <!-- Discord Link. -->
                <!-- <span class="link-block">
                  <a href="https://discord.gg/4Gnw7eTEZR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-discord"></i>
                    </span>
                    <span>Discord</span>
                  </a>
                </span> -->

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="column is-full">
        <video controls muted loop autoplay width="100%">
          <source src="static/videos/main.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Main Figure. -->
      <h2 class="title is-3"></h2>
      <div class="content has-text-justified">
        <img src="img/overview.png" width="100%" alt="GUI-world Overview" class="responsive-image">
        <!-- <img src="img/radar.jpg" width="100%" alt="GUI-world Benchmark Overview" class="responsive-image"> -->
        <md-block>
          In this work, we introduce LIVEVQA, a new dataset and benchmark for evaluating Multimodal Large Language Models (MLLMs) on their ability to understand and reason about up-to-date visual information. Specifically, there are three-fold major contributions:
          <ol>
            <li><b>A Novel Dataset.</b> We introduce LIVEVQA, the first dataset of its kind, featuring 107,143 samples across 12 categories, specifically designed to test how MLLMs handle visual information beyond their training data cutoff and how they can be updated with new knowledge.</li>
            <li><b>Comprehensive Benchmarking and Analysis.</b> We conducted extensive benchmarking of 17 state-of-the-art MLLMs, revealing significant performance gaps on content beyond their knowledge cutoff. Our findings show that tool-use or agentic visual seeking frameworks can drastically improve performance by an average of 327%.</li>
            <li><b>Efficient Knowledge Updating Insights.</b> We explored parameter-efficient fine-tuning (PEFT) methods, demonstrating that MLLMs can be efficiently updated with new visual knowledge within a single epoch. While this can impact visual perception, it can enhance knowledge-intensive capabilities, and we provide insights into balancing adapter capacity and model capability.</li>
          </ol>
        </md-block>
      </div>
      <!--/ Main Figure. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-wdith">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <md-block>
              The visual world around us constantly evolves, from real-time news and social media trends to global infrastructure changes visible through satellite imagery and augmented reality enhancements. However, Multimodal Large Language Models (MLLMs), which automate many tasks, struggle to stay current, limited by the cutoff dates in their fixed training datasets.
              To quantify this stagnation, we introduce LiveVQA, the first-of-its-kind dataset featuring 107,143 samples and 12 categories data specifically designed to support research in both seeking and updating with live visual knowledge.
              Drawing from recent news articles, video platforms, and academic publications in April 2024-May 2025, LiveVQA enables evaluation of how models handle latest visual information beyond their knowledge boundaries and how current methods help to update them. 
              Our comprehensive benchmarking of 17 state-of-the-art MLLMs reveals significant performance gaps on content beyond knowledge cutoff, and tool-use or agentic visual seeking framework drastically gain an average of 327% improvement. 
              Furthermore, we explore parameter-efficient fine-tuning (PEFT) methods to update MLLMs with new visual knowledge.
              We dive deeply to the critical balance between adapter capacity and model capability when updating MLLMs with new visual knowledge. 
            </md-block>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Environment Infrastructure. -->
      <h2 class="title is-3">LIVEVQA Dataset Construction</h2>
      <div class="content has-text-justified">
        <img src="img/builder.png" width="100%" alt="environment infrastructure" class="responsive-image">
        <md-block>
          We introduce LIVEVQA, a first-of-its-kind dataset containing fresh visual content and corresponding question-answer pairs, aimed at benchmarking and advancing Multimodal Large Language Models (MLLMs) in seeking and updating live visual knowledge. The visual content is sourced from recent international news articles, YouTube videos, and academic papers spanning from April 2024 to early May 2025.
          The construction of the LIVEVQA dataset primarily follows a multi-stage LLM/MLLM-in-the-loop pipeline with rigorous filtering and human validation:
          <ol>
            <li><b>Raw Data Collection from Diverse Sources</b>: This stage involves collecting recent visual and textual data. For news articles, this includes URL and headline filtering, image selection based on size and relevance (enhanced using GPT-4.1 to ensure strong correlation with events), and semantic deduplication. For videos (from YouTube), it involves preprocessing (restricting to English, max 10 mins, with subtitles), subtitle-based segmentation using an LLM, initial keyframe identification (using UVD and perceptual hashing for deduplication), and LLM-driven selection of top-K relevant keyframes. For academic papers (from arXiv), it includes extracting titles, abstracts, authors, images, and captions, followed by key image selection prioritizing architectural diagrams and key findings, avoiding common visualizations.</li>
            <li><b>Visual Question Answering (VQA) Generation and Filtering</b>: This stage constructs two levels of questions. Level 1 questions target basic visual entity recognition (e.g., locations, persons, time) based on filtered images and metadata, with GPT-4.1 used to filter out unqualified QAs (e.g., those with overly brief answers or simple labels). Level 2 questions are more complex, requiring multi-hop cross-modal reasoning using the full image context and related textual information, covering seven types (location, person, organization, time, event, count, reason); these are also generated and filtered by GPT-4.1 to ensure answer verifiability. All LLM/MLLM-assisted processes undergo human validation with a high pass rate.</li>
        </ol>
      </md-block>
      </div>
      <!--/ Environment Infrastructure. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">LIVEVQA Dataset Statistics</h2>
      <md-block>
        Below we present an overview of the main statistics of LIVEVQA, showcasing its composition across different data sources and splits. LIVEVQA contains a total of 28,488 unique images and 107,143 questions.
      </md-block>
      <div class="column is-full-width interpolation-panel">
        <div class="table-container" style="margin-bottom: 20px;"> <table class="table is-hoverable is-striped is-bordered" style="margin: 0 auto; background-color: rgba(0,0,0,0); width: 100%; text-align: center;">
            <thead>
              <tr>
                <th style="text-align: left;">Category</th>
                <th>Images</th>
                <th>#Question</th>
                <th>Level 1</th>
                <th>Level 2</th>
                <th>Avg. Len.</th>
                <th>Purpose</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="font-weight: bold; text-align: left;">News Article</td>
                <td>7,579</td>
                <td>38,809</td>
                <td>7,579</td>
                <td>31,230</td>
                <td>749</td>
                <td>-</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">YouTube Videos</td>
                <td>11,948</td>
                <td>43,168</td>
                <td>11,948</td>
                <td>31,220</td>
                <td>311</td>
                <td>-</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">Academic Paper</td>
                <td>8,961</td>
                <td>25,166</td>
                <td>9,456</td>
                <td>16,205</td>
                <td>597</td>
                <td>-</td>
              </tr>
              <tr>
                <td colspan="7" style="border-top: 2px solid #dbdbdb;"></td> </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">Avg. per Sample</td>
                <td>1</td>
                <td>3.86</td>
                <td>1</td>
                <td>2.86</td>
                <td>517</td>
                <td>-</td>
              </tr>
              <tr>
                <td colspan="7" style="border-top: 2px solid #dbdbdb;"></td> </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">Test Split</td>
                <td>1,500</td>
                <td>3,000</td>
                <td>1,500</td>
                <td>1,500</td>
                <td>544</td>
                <td>Exp. 1</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left;">Training Split</td>
                <td>26,988</td>
                <td>104,143</td>
                <td>26,988</td>
                <td>77,150</td>
                <td>496</td>
                <td>Exp. 2</td>
              </tr>
            </tbody>
          </table>
        </div>

        <div style="text-align: center;">
            <img src="img/static.png" alt="Dataset Statistics Distributions" style="width: 100%; max-width: 700px; margin-top: 10px; display: block; margin-left: auto; margin-right: auto;"> <p style="font-size: 0.9em; color: #555; margin-top: 5px; text-align: center;">Figure 4: (Left) Image size distribution in YouTube image filtering pipeline. (Right) Textual context length distribution for each question.</p>
        </div>
      </div>
      </div>
  </section>

  <section>
    <div class="container is-max-desktop">

      <h2 class="title is-3">Benchmark Results for LIVEVQA</h2>
      <md-block>
        We conducted a comprehensive benchmark of 17 state-of-the-art Multimodal Large Language Models (MLLMs) to evaluate their capabilities in seeking and updating live visual knowledge. The evaluation was performed on the LIVEVQA dataset, which includes content from recent news articles, YouTube videos, and academic papers. Performance was measured for Level 1 (visual entity recognition) and Level 2 (deeper visual knowledge reasoning) questions, with and without various search augmentation methods.
        Key findings indicate that current MLLMs struggle significantly with visual knowledge beyond their training cutoff, but performance is drastically improved with the use of multimodal search tools.
      </md-block>
    </div>
    <div class="cover" id="contentCover">
      <div class="container-t">
        <div class="row">
          <div class="col-md-12">
            <div class="infoCard">
              <div class="infoBody">
                <div class="tabs is-centered example_lst">
                  <ul>
                    <li class="is-active"><a title="Overall Accuracy">Overall Accuracy (Table 2)</a></li>
                    <li><a title="News Subset Details">News Subset - Detailed Categories (Table 3)</a></li>
                    <li><a title="Video Subset Details">Video Subset - Detailed Categories (Table 7)</a></li>
                  </ul>
                </div>
                <script type="text/javascript">
                  document.querySelectorAll(".example_lst li").forEach(e => {
                    e.addEventListener("click", Click_1)
                  })

                  function Click_1(eve) {
                    const iTxt = eve.target.closest('a').title; // Use title attribute for reliable selection
                    document.querySelectorAll(".example_lst li").forEach(v_li => {
                      if (iTxt === v_li.querySelector('a').title) {
                        v_li.classList.add("is-active");
                      } else {
                        v_li.classList.remove("is-active");
                      }
                    });
                    document.querySelectorAll(".lib_examples").forEach(block => {
                      block.style.display = (block.title === iTxt) ? 'block' : 'none';
                    });
                  }
                  // Initialize first tab
                  document.addEventListener('DOMContentLoaded', () => {
                    const firstTabLink = document.querySelector(".example_lst li.is-active a");
                    if (firstTabLink) {
                        Click_1({ target: firstTabLink });
                    }
                  });
                </script>

                <div title="Overall Accuracy" class="lib_examples" id="BoardPanel1" style="display: block;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Accuracy (%) of visual factuality seeking benchmark in open-ended format. (Avg. = Average)
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size: 0.85em;">
                      <thead>
                        <tr>
                          <th rowspan="2">Model</th>
                          <th rowspan="2">Cutoff</th>
                          <th colspan="4">Level 1</th>
                          <th colspan="4">Level 2</th>
                        </tr>
                        <tr>
                          <th>News</th>
                          <th>Video</th>
                          <th>Arxiv</th>
                          <th>Avg.</th>
                          <th>News</th>
                          <th>Video</th>
                          <th>Arxiv</th>
                          <th>Avg.</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="10" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w.o. Search</td></tr>
                        <tr><td>GPT-4.1</td><td>Jun. 2024</td><td>27.0</td><td>22.0</td><td>0.4</td><td><strong>16.5</strong></td><td>5.2</td><td>7.2</td><td>0.2</td><td>3.0</td></tr>
                        <tr><td>GPT-4.1-mini</td><td>Jun. 2024</td><td>24.6</td><td>19.6</td><td>0.2</td><td>14.8</td><td>4.0</td><td>7.8</td><td>0.4</td><td>4.0</td></tr>
                        <tr><td>GPT-4.1-nano</td><td>Jun. 2024</td><td>13.0</td><td>13.0</td><td>0.0</td><td>8.6</td><td>2.2</td><td>6.0</td><td>0.4</td><td>2.9</td></tr>
                        <tr><td>Gemini-2.5-Flash</td><td>Jan. 2025</td><td>25.8</td><td>18.4</td><td>0.8</td><td>15.0</td><td>4.6</td><td>4.4</td><td>4.0</td><td><strong>4.3</strong></td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>Jan. 2025</td><td>28.0</td><td>17.4</td><td>0.6</td><td><strong>15.3</strong></td><td>4.4</td><td>2.4</td><td>1.2</td><td>2.7</td></tr>
                        <tr><td>Gemma-3-27B-It</td><td>Aug. 2024</td><td>21.0</td><td>16.4</td><td>1.0</td><td>12.8</td><td>3.8</td><td>4.6</td><td>6.2</td><td><strong>4.9</strong></td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>Oct. 2024</td><td>26.2</td><td>16.4</td><td>0.6</td><td>14.3</td><td>2.2</td><td>4.4</td><td>4.4</td><td>3.7</td></tr>
                        <tr><td>Qwen-2.5-VL-7B-Instruct</td><td>Unknown</td><td>20.2</td><td>13.4</td><td>0.2</td><td>11.3</td><td>3.8</td><td>5.4</td><td>2.0</td><td>3.7</td></tr>
                        <tr><td>Qwen-2.5-VL-32B-Instruct</td><td>Unknown</td><td>25.2</td><td>16.4</td><td>0.4</td><td>14.0</td><td>4.2</td><td>5.6</td><td>1.2</td><td>3.7</td></tr>
                        <tr><td>Qwen-2.5-VL-72B-Instruct</td><td>Unknown</td><td>12.4</td><td>9.4</td><td>0.0</td><td>7.3</td><td>1.4</td><td>3.6</td><td>3.6</td><td>2.9</td></tr>
                        <tr><td>Llama-4-Scout</td><td>Aug. 2024</td><td>20.6</td><td>16.4</td><td>0.0</td><td>12.1</td><td>4.0</td><td>5.0</td><td>2.8</td><td>3.9</td></tr>
                        <tr><td>Llama-4-Maverick</td><td>Aug. 2024</td><td>20.2</td><td>19.0</td><td>0.6</td><td><strong>13.3</strong></td><td>5.8</td><td>6.0</td><td>5.2</td><td><strong>5.7</strong></td></tr>
                        <tr><td colspan="10" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Text Search</td></tr>
                        <tr><td>GPT-4.1</td><td>Jun. 2024</td><td>25.0</td><td>21.4</td><td>0.6</td><td><strong>15.6</strong></td><td>3.6</td><td>5.6</td><td>3.8</td><td><strong>4.3</strong></td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>Jan. 2025</td><td>17.6</td><td>9.2</td><td>0.2</td><td>9.0</td><td>2.0</td><td>1.6</td><td>1.0</td><td>1.5</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>Oct. 2024</td><td>24.6</td><td>16.6</td><td>0.0</td><td>13.7</td><td>2.0</td><td>3.6</td><td>4.8</td><td>3.5</td></tr>
                        <tr><td colspan="10" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Native Image Search</td></tr>
                        <tr><td>GPT-03</td><td>Jun. 2024</td><td><strong>33.6</strong></td><td><strong>33.6</strong></td><td>2.6</td><td><strong>23.3</strong></td><td><strong>14.6</strong></td><td><strong>14.9</strong></td><td><strong>17.8</strong></td><td><strong>15.8</strong></td></tr>
                        <tr><td colspan="10" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. MM-Search [Jiang et al., 2024]</td></tr>
                        <tr><td>GPT-4.1</td><td>Jun. 2024</td><td><strong>42.0</strong></td><td><strong>36.1</strong></td><td><strong>22.0</strong></td><td><strong>33.4</strong></td><td><strong>27.2</strong></td><td><strong>15.2</strong></td><td><strong>48.8</strong></td><td><strong>30.4</strong></td></tr>
                      </tbody>
                    </table>
                  </div>
                </div>

                <div title="News Subset Details" class="lib_examples" id="BoardPanel2" style="display: none;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Comparison performance on detailed categories in News subset. (Loc.=Location, Per.=Person, Org.=Organization, Eve.=Event, Obj.=Object, Cou.=Count, Rea.=Reason, Avg.=Average)
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size:0.8em;">
                      <thead>
                        <tr>
                          <th rowspan="2">Model</th>
                          <th colspan="6">Level 1 (News Subset)</th>
                          <th colspan="8">Level 2 (News Subset)</th>
                        </tr>
                        <tr>
                          <th>Loc.</th><th>Per.</th><th>Org.</th><th>Eve.</th><th>Obj.</th><th>Avg.</th>
                          <th>Loc.</th><th>Per.</th><th>Org.</th><th>Time</th><th>Cou.</th><th>Rea.</th><th>Eve.</th><th>Avg.</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w.o. Search</td></tr>
                        <tr><td>GPT-4.1</td><td><strong>50.72</strong></td><td>15.19</td><td>35.89</td><td>27.03</td><td>6.28</td><td>28.81</td><td>0.00</td><td>1.75</td><td><strong>11.68</strong></td><td>3.82</td><td>7.84</td><td>1.63</td><td>0.00</td><td>5.05</td></tr>
                        <tr><td>GPT-4.1-mini</td><td>33.33</td><td>10.91</td><td>45.59</td><td>11.86</td><td>19.23</td><td>24.60</td><td>0.00</td><td>3.57</td><td>8.82</td><td>0.00</td><td>10.24</td><td>0.00</td><td>0.00</td><td>4.00</td></tr>
                        <tr><td>GPT-4.1-Nano</td><td>16.16</td><td>3.64</td><td>30.88</td><td>3.39</td><td>13.00</td><td>13.00</td><td>0.00</td><td>0.00</td><td>4.41</td><td>1.54</td><td>3.94</td><td>0.83</td><td>0.00</td><td>2.20</td></tr>
                        <tr><td>Gemini-2.5-Flash</td><td>26.26</td><td>37.27</td><td>35.29</td><td>7.63</td><td>25.80</td><td>25.80</td><td>0.00</td><td>3.57</td><td>1.47</td><td>3.85</td><td>8.66</td><td>4.17</td><td>0.00</td><td>4.60</td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>23.23</td><td><strong>46.36</strong></td><td>35.29</td><td>10.17</td><td>28.00</td><td>28.00</td><td>3.57</td><td>0.00</td><td>5.88</td><td>3.08</td><td>3.94</td><td>6.67</td><td>0.00</td><td>4.40</td></tr>
                        <tr><td>Gemma-3-27B-IT</td><td>24.24</td><td>15.45</td><td>38.24</td><td>8.47</td><td>21.00</td><td>21.00</td><td>3.57</td><td>0.00</td><td>8.82</td><td>1.54</td><td>7.87</td><td>0.00</td><td>0.00</td><td>3.80</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>26.20</td><td>38.38</td><td>10.00</td><td>14.41</td><td>26.20</td><td>26.20</td><td>0.00</td><td>0.00</td><td>4.41</td><td>2.31</td><td>1.57</td><td>2.50</td><td>0.00</td><td>2.20</td></tr>
                        <tr><td>Qwen-2.5-VL-7B</td><td>23.23</td><td>21.15</td><td>30.88</td><td>12.71</td><td>20.20</td><td>20.20</td><td>0.00</td><td>0.00</td><td>4.41</td><td>1.54</td><td>7.09</td><td>4.17</td><td>0.00</td><td>3.80</td></tr>
                        <tr><td>Qwen-2.5-VL-32B</td><td>33.33</td><td>18.18</td><td>30.88</td><td>18.64</td><td>25.20</td><td>25.20</td><td>0.00</td><td>0.00</td><td>7.35</td><td>2.31</td><td>6.30</td><td>4.17</td><td>0.00</td><td>4.20</td></tr>
                        <tr><td>Qwen-2.5-VL-72B</td><td>12.50</td><td>6.36</td><td>15.15</td><td>8.47</td><td>12.40</td><td>12.40</td><td>0.00</td><td>0.00</td><td>4.41</td><td>0.77</td><td>1.57</td><td>0.83</td><td>0.00</td><td>1.40</td></tr>
                        <tr><td>Llama-4-Scout</td><td>26.26</td><td>13.64</td><td>35.29</td><td>8.47</td><td>20.60</td><td>20.60</td><td>3.57</td><td>0.00</td><td>4.41</td><td>3.08</td><td>9.45</td><td>0.00</td><td>0.00</td><td>4.00</td></tr>
                        <tr><td>Llama-4-Maverick</td><td>20.20</td><td>19.09</td><td>36.76</td><td>5.93</td><td>20.20</td><td>20.20</td><td>0.00</td><td>0.00</td><td><strong>10.29</strong></td><td>2.31</td><td><strong>13.39</strong></td><td>1.67</td><td>0.00</td><td><strong>5.80</strong></td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Text Search</td></tr>
                        <tr><td>GPT-4.1</td><td>34.62</td><td>13.56</td><td>48.53</td><td>2.73</td><td>25.00</td><td>25.00</td><td>5.88</td><td>3.57</td><td>5.88</td><td>3.85</td><td>4.72</td><td>0.83</td><td>0.00</td><td>3.60</td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>18.18</td><td>10.17</td><td>29.41</td><td>12.73</td><td>17.60</td><td>17.60</td><td>0.00</td><td>3.57</td><td>4.41</td><td>1.54</td><td>2.36</td><td>1.67</td><td>0.00</td><td>2.00</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>23.08</td><td>18.64</td><td>40.38</td><td>6.36</td><td>24.60</td><td>24.60</td><td>0.00</td><td>5.88</td><td>1.47</td><td>1.54</td><td>3.15</td><td>0.83</td><td>0.00</td><td>2.00</td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Native Image Search</td></tr>
                        <tr><td>GPT-03</td><td><strong>47.47</strong></td><td>23.73</td><td><strong>57.35</strong></td><td><strong>47.12</strong></td><td>33.60</td><td><strong>33.60</strong></td><td>0.00</td><td>17.86</td><td><strong>20.59</strong></td><td>7.69</td><td><strong>17.32</strong></td><td><strong>17.50</strong></td><td><strong>10.00</strong></td><td><strong>14.60</strong></td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. MM-Search [Jiang et al., 2024]</td></tr>
                        <tr><td>GPT-4.1</td><td><strong>50.00</strong></td><td>35.78</td><td><strong>55.88</strong></td><td><strong>42.86</strong></td><td>42.00</td><td><strong>42.00</strong></td><td><strong>15.50</strong></td><td><strong>23.53</strong></td><td><strong>30.88</strong></td><td><strong>42.52</strong></td><td><strong>20.00</strong></td><td><strong>46.43</strong></td><td>0.00</td><td><strong>27.20</strong></td></tr>
                      </tbody>
                    </table>
                  </div>
                </div>

                 <div title="Video Subset Details" class="lib_examples" id="BoardPanel3" style="display: none;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Performance on detailed categories in Video subset. (Loc.=Location, Per.=Person, Org.=Organization, Eve.=Event, Obj.=Object, Cou.=Count, Rea.=Reason, Avg.=Average)
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size:0.8em;">
                      <thead>
                        <tr>
                          <th rowspan="2">Model</th>
                          <th colspan="6">Level 1 (Video Subset)</th>
                          <th colspan="8">Level 2 (Video Subset)</th>
                        </tr>
                        <tr>
                          <th>Loc.</th><th>Per.</th><th>Org.</th><th>Eve.</th><th>Obj.</th><th>Avg.</th>
                          <th>Loc.</th><th>Per.</th><th>Org.</th><th>Time</th><th>Cou.</th><th>Rea.</th><th>Eve.</th><th>Avg.</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w.o. Search</td></tr>
                        <tr><td>GPT-4.1</td><td>26.58</td><td>8.33</td><td><strong>40.85</strong></td><td>7.77</td><td><strong>32.23</strong></td><td><strong>22.00</strong></td><td><strong>8.51</strong></td><td>3.45</td><td>5.56</td><td>6.32</td><td><strong>11.20</strong></td><td>5.65</td><td>4.55</td><td><strong>7.20</strong></td></tr>
                        <tr><td>GPT-4.1-mini</td><td>21.52</td><td>13.54</td><td>30.99</td><td>4.85</td><td><strong>30.58</strong></td><td>19.60</td><td>2.13</td><td>3.45</td><td><strong>12.96</strong></td><td>6.32</td><td><strong>15.20</strong></td><td>3.23</td><td>4.55</td><td><strong>7.80</strong></td></tr>
                        <tr><td>GPT-4.1-nano</td><td>15.19</td><td>1.04</td><td>28.17</td><td>4.85</td><td>19.01</td><td>13.00</td><td>0.00</td><td>0.00</td><td>5.56</td><td>6.32</td><td>14.40</td><td>2.42</td><td>0.00</td><td>6.00</td></tr>
                        <tr><td>Gemini-2.5-Flash</td><td>18.99</td><td>27.08</td><td>29.58</td><td>4.85</td><td>18.18</td><td>18.40</td><td>0.00</td><td>3.45</td><td>1.85</td><td>4.21</td><td>11.20</td><td>0.81</td><td>4.55</td><td>4.40</td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>8.86</td><td>25.00</td><td>32.39</td><td>6.80</td><td>19.01</td><td>17.40</td><td>0.00</td><td>0.00</td><td>1.85</td><td>2.11</td><td>5.60</td><td>1.61</td><td>0.00</td><td>2.40</td></tr>
                        <tr><td>Gemma-3-27B-IT</td><td>13.92</td><td>14.58</td><td>33.80</td><td>3.88</td><td>21.49</td><td>16.40</td><td>0.00</td><td>0.00</td><td>5.56</td><td>4.21</td><td>10.40</td><td>1.61</td><td>4.55</td><td>4.60</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>18.99</td><td>7.29</td><td>29.58</td><td>6.80</td><td>23.97</td><td>16.40</td><td>2.13</td><td>0.00</td><td>1.85</td><td>4.21</td><td>7.20</td><td>4.84</td><td>4.55</td><td>4.40</td></tr>
                        <tr><td>Qwen-2.5-VL-7B</td><td>12.66</td><td>10.42</td><td>25.35</td><td>4.85</td><td>16.53</td><td>13.40</td><td>2.13</td><td>0.00</td><td>5.56</td><td>3.16</td><td>14.40</td><td>1.61</td><td>0.00</td><td>5.40</td></tr>
                        <tr><td>Qwen-2.5-VL-32B</td><td>16.46</td><td>10.42</td><td>32.39</td><td>4.85</td><td>22.31</td><td>16.40</td><td>0.00</td><td>0.00</td><td>5.56</td><td>6.32</td><td>9.60</td><td>4.84</td><td>4.55</td><td>5.60</td></tr>
                        <tr><td>Qwen-2.5-VL-72B</td><td>10.13</td><td>3.12</td><td>18.31</td><td>1.94</td><td>14.88</td><td>9.40</td><td>0.00</td><td>0.00</td><td>7.41</td><td>3.16</td><td>5.60</td><td>2.42</td><td>4.55</td><td>3.60</td></tr>
                        <tr><td>Llama-4-Scout</td><td>16.46</td><td>13.54</td><td>26.76</td><td>7.77</td><td>20.66</td><td>16.40</td><td>2.13</td><td>0.00</td><td>7.41</td><td>4.21</td><td>10.40</td><td>1.61</td><td>4.55</td><td>5.00</td></tr>
                        <tr><td>Llama-4-Maverick</td><td>18.99</td><td>14.58</td><td>38.03</td><td>8.74</td><td>20.66</td><td>19.00</td><td>2.13</td><td>3.45</td><td>3.70</td><td>4.21</td><td>15.20</td><td>2.42</td><td>0.00</td><td>6.00</td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Text Search</td></tr>
                        <tr><td>GPT-4.1</td><td>13.92</td><td>6.25</td><td>30.05</td><td>3.56</td><td>22.59</td><td>14.60</td><td>2.84</td><td>0.00</td><td>3.09</td><td>3.86</td><td>6.67</td><td>2.42</td><td>3.03</td><td>3.73</td></tr>
                        <tr><td>Gemini-2.5-Pro</td><td>1.69</td><td>1.39</td><td>19.72</td><td>2.91</td><td>8.54</td><td>6.53</td><td>0.00</td><td>0.00</td><td>0.62</td><td>1.40</td><td>3.20</td><td>0.00</td><td>1.52</td><td>1.20</td></tr>
                        <tr><td>Claude-3.7-Sonnet</td><td>8.02</td><td>4.17</td><td>14.55</td><td>2.59</td><td>12.95</td><td>8.33</td><td>1.42</td><td>0.00</td><td>1.23</td><td>1.40</td><td>3.73</td><td>0.54</td><td>0.00</td><td>1.60</td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. Native Image Search</td></tr>
                        <tr><td>GPT-o3</td><td><strong>37.97</strong></td><td>19.79</td><td><strong>43.66</strong></td><td><strong>22.33</strong></td><td><strong>46.28</strong></td><td><strong>33.60</strong></td><td>8.51</td><td><strong>10.34</strong></td><td><strong>12.96</strong></td><td><strong>11.58</strong></td><td><strong>29.60</strong></td><td><strong>25.00</strong></td><td><strong>18.18</strong></td><td><strong>19.40</strong></td></tr>
                        <tr><td colspan="15" style="font-weight:bold; text-align:left; background-color:#f0f0f0;">w. MM-Search [Jiang et al., 2024]</td></tr>
                        <tr><td>GPT-4.1</td><td><strong>29.11</strong></td><td><strong>31.58</strong></td><td><strong>49.30</strong></td><td><strong>21.36</strong></td><td><strong>38.84</strong></td><td><strong>33.00</strong></td><td><strong>13.68</strong></td><td><strong>17.02</strong></td><td>10.34</td><td>11.11</td><td><strong>26.40</strong></td><td>9.68</td><td>4.55</td><td><strong>15.20</strong></td></tr>
                      </tbody>
                    </table>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="Empirical Result">
    <div class="container is-max-desktop">
      <div class="featurecard-container">
        <!-- Trustworthiness and Utility -->
        <h1 class="title">Empirical Results</h1>
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot Settings</h2>
          </div>
          <div class="description">
            <p>
            <ol>
              <li><b>Commercial ImageLLMs, notably GPT-4V and GPT-4o, consistently outperform open-source
                  VideoLLMs in zero-shot settings.
                  GPT-4o exhibits superior performance across all GUI scenarios in complex tasks, reflected in its
                  high scores in both multiple-choice and free-form queries, with an average of 84.8% and 3.573.
                  Similarly, Gemini demonstrates strong capabilities in captioning and descriptive tasks within
                  software and iOS environments, scoring 2.836 and 2.936, respectively.
                  Further analysis reveals that GPT-4V excels in applications with minimal textual content and
                  simple layouts, such as TikTok, health apps, and GitHub.
                  In contrast, its performance drops in more intricate applications like Microsoft ToDo and XR
                  software.
                  As for VideoLLMs, their significantly poorer performance is attributed to two main factors:
                  their inability to accurately interpret GUI content from user inputs and a lack of sufficient
                  GUI-oriented pretraining, which is evident from their inadequate performance in basic captioning
                  and description tasks.</li>
            </ol>
            </p>
          </div>
        </div>

        <!-- Alignment of LLMs -->
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Performance Variate in Different GUI Scenarios</h2>
          </div>
          <div class="description">
            <p>
              GPT-4V and Gemini excel in common scenarios such as mobile and website interfaces but show marked
              deficiencies in more complex GUI environments like XR and multi-window interactions, across both
              captioning and intricate tasks.
              This performance gap highlights a significant shortfall in understanding environments where GUI
              elements are scattered and demand sophisticated interpretation.
              It emphasizes the critical need for specialized benchmarks and datasets tailored to these complex
              GUI scenarios, which is essential for enhancing the GUI-oriented capabilities of MLLMs, paving the
              way for them to become truly reliable and high-performing general control agents.
            </p>
          </div>
        </div>

        <!-- Performance Gap in Trustworthiness -->
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Keyframe Selection is Important for GUI-oriented Tasks</h2>
          </div>
          <div class="description">
            <p>
              Across both basic tasks such as captioning and more complex tasks like prediction and reasoning,
              significant variations are evident among keyframe selection methods.
              GPT-4V and Gemini significantly benefit from using random-selected and human-selected keyframes,
              scoring approximately 0.2-0.3 points higher in both captioning and free-form tasks than those using
              programmatic extraction.
              This suggests that traditional keyframe technologies, designed for natural videos, are less
              effective for detecting essential GUI operations, particularly when subtle movements like mouse
              clicks and dynamic changes are involved.
              Conversely, the difference in performance is relatively smaller in Qwen-VL-Max, indicating that
              while keyframe selection methods are crucial for models proficient in GUI content, they exert less
              influence on less capable models.
            </p>
          </div>
        </div>


        <!-- Transparency in Trustworthiness -->
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>Dynamic GUI Tasks Continue to Challenge MLLMs</h2>
          </div>
          <div class="description">
            <p>
              In the fine-grained tasks, GPT-4V and GPT-4o excel with static GUI content and prediction tasks over
              image sequences but struggle with providing detailed descriptions for entire videos and dynamic GUI
              content.
              This discrepancy is attributed to minor variations in GUI that significantly impact descriptions.
              Enhancing the number of keyframes and the granularity of perception might mitigate these issues.
              Among VideoLLMs, ChatUnivi excels in conversational tasks by effectively leveraging contextual
              nuances, particularly in subsequent rounds, yet it underperforms in GUI-oriented captioning tasks.
              In contrast, GUI-Vid demonstrates proficiency in sequential tasks but falls short in both captioning
              and static content handling.
              This gap is linked to deficiencies in GUI-Vid’s pretraining, which lacked comprehensive GUI content
              crucial for effective vision-text alignment, as evidenced by its poor performance and an instruction
              tuning process also failed to fully address these shortcomings.</p>

          </div>
        </div>
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>Vision Perception is Important for Sequential GUI Tasks</h2>
          </div>
          <div class="description">
            <p>
              Integrating detailed textual information slightly outperforms purely vision-based inputs or detailed
              captions, akin to a Chain of Thought (CoT) setting.
              Surprisingly, GPT-4V excels in caption and prediction tasks with just detailed captions, providing
              insights on enhancing specific GUI-oriented tasks through additional textual information.
              However, it still falls short in more challenging tasks, such as retrieving static or dynamic
              content. This underscores the critical role of visual perception in GUI environments, where even
              minor changes can significantly impact outcomes.</p>

          </div>
        </div>
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned on GUI-World</h2>
          </div>
          <div class="description">
            <p>
              As a pioneering study in training VideoLLMs as screen agents, GUI-Vid significantly outperforms the
              baseline model, showing an average improvement of 30% across various tasks and GUI scenarios, even
              surpassing the commercial ImageLLM, Qwen-VL-Max.
              This enhancement is particularly notable in captioning and prediction over image sequences, where
              GUI-Vid matches the performance of GPT-4V and Gemini-Pro. Our two-stage progressive fintuning
              significantly enhances the performance in all GUI scenarios.
              Remarkably, GUI-Vid scored 3.747 in caption tasks within the XR scenario, highlighting its potential
              in XR applications and the high-quality annotations provided by our dataset.
              However, in Multiple-Choice QA and Chatbot tasks, GUI-Vid still lags behind industry leaders like
              GPT-4V and Gemini-Pro, a discrepancy likely due to the baseline LLM’s weaker performance and the
              challenges of instruction-based fine-tuning.</p>

          </div>
        </div>
        <div class="feature_1x1">

          <div class="featurecard">
            <h2>Upper Bound of GUI-oriented Capability with More Keyframes and High Resolution</h2>
          </div>
          <div class="description">
            <p>
              Our two ablation studies during the fine-tuning phase demonstrate that utilizing GUI image-text
              captioning data significantly enhances the model's preliminary understanding of GUI elements,
              outperforming training that relies solely on videos.
              Additionally, an increased number of keyframes correlates with improved performance across various
              scenarios, notably in environments featuring multiple windows and software applications.
              Further evidence reveals that higher image resolutions substantially boost task performance, both
              basic and complex, for GPT-4o.
              These findings underscore the potential for further developing a more robust GUI Agent.</p>

          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="Acknoledgement">
    <div class="container is-max-desktop content">
      <h1 class="supportTitle">Acknowledgement</h1>
      <md-block>
        Many thanks to Yinuo Liu, Zhengyan Fu, Shilin Zhang, Yu, Tianhe Gu, Haokuan Yuan, and Junqi Wang for their
        invalueble effort in this project.
        This project is based on methodologies and code presented in
        [Videochat2](https://github.com/OpenGVLab/Ask-Anything).
        This website is based on templates in [TrustLLM](https://trustllmbenchmark.github.io/TrustLLM-Website/) and
        [OSWorld](https://os-world.github.io/).
      </md-block>
    </div>
  </section>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h1 class="title">BibTeX</h1>
      <pre><code>@misc{chen2024guiworld,
        title={Seeking and Updating with Live Visual Knowledge}, 
        author={Dongping Chen and Yue Huang and Siyuan Wu and Jingyu Tang and Liuyi Chen and Yilin Bai and Zhigang He and Chenlong Wang and Huichi Zhou and Yiqiang Li and Tianshuo Zhou and Yue Yu and Chujie Gao and Qihui Zhang and Yi Gui and Zhen Li and Yao Wan and Pan Zhou and Jianfeng Gao and Lichao Sun},
        year={2024},
        eprint={2406.10819},
  }</code></pre>
    </div>
  </section>
  <div class="content">
    <div id="supportContainer">
      <h1 class="supportTitle">GUI-World Team</h1>
      <br>

      <div id="logoContainer">
        <img src="img/logos/HUST.png" alt="School 1" class="schoolLogo">
        <img src="img/logos/Lehigh-University-logo.png" alt="School 2" class="schoolLogo">
        <img src="img/logos/ND.png" alt="School 3" class="schoolLogo">
        <!-- <img src="img/logos/ND.png" alt="School 3" class="schoolLogo"> -->
        <img src="img/logos/Microsoft.png" alt="School 4" class="schoolLogo">

      </div>


    </div>
  </div>
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.

            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>