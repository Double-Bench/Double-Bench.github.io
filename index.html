<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?</title>
  <script type="module" src=""></script>
 

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./img/robot_metrics2_transparent.png">

  <link rel="stylesheet" href="./stylesheets/layout.css">
  <link rel="stylesheet" href="./stylesheets/index.css">
  <link rel="stylesheet" href="./bowe_componets/css/bootstrap.table.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link href="./static/css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="./static/css/custom.css" media="screen" rel="stylesheet" type="text/css" />
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://dongping-chen.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://trustgen.github.io">
              Trustworhty Generative Models
            </a>
            <a class="navbar-item" href="https://mllm-judge.github.io">
              MLLM-as-a-Judge
            </a>
            <a class="navbar-item" href="https://multiref.github.io">
              MultiRef
            </a>
            <a class="navbar-item" href="https://livevqa.github.io">
              LiveVQA
            </a>
            <a class="navbar-item" href="https://gui-world.github.io/">
              GUI-World
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?
            </h1>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://dongping-chen.github.io/">Dongping
                  Chen</a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup><sup style="color:#c9892e;">†</sup>,</span>
              <span class="author-block"><a href="https://howiehwong.github.io/">Yue Huang</a><sup style="color:#2bff32;">2</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://siyuan-5.github.io/">Siyuan Wu<a></a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block"><a href="https://jingyuhhh.github.io/">Jingyu Tang</a><sup style="color:#ec8bfd;">1</sup><sup style="color:#ff2b67;">*</sup>,</span>
              <span class="author-block">Liuyi Chen<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yilin Bai<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Zhigang He<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Chenlong Wang<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://huichizhou.github.io/">Huichi Zhou</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yiqiang Li<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Tianshuo Zhou<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yue Yu<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://flossiee.github.io/">Chujie Gao</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://mask-hui.github.io/">Qihui Zhang</a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Yi Gui<sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block">Zhen Li<sup style="color:#ec8bfd;">1</sup>,</span>

              <span class="author-block"><a href="http://wanyao.me/"><b>Yao Wan</b></a><sup style="color:#ec8bfd;">1</sup><sup style="color:#c9892e;">†</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=cTpFPJgAAAAJ&hl=en"><b>Pan
                  Zhou</b></a><sup style="color:#ec8bfd;">1</sup>,</span>
              <span class="author-block"><a href="https://scholar.google.com/citations?user=CQ1cqKkAAAAJ&hl=en"><b>Jianfeng
                  Gao</b></a><sup style="color:#fabb55;">3</sup>,</span>
              <span class="author-block"><a href="https://lichao-sun.github.io/"><b>Lichao Sun</b></a><sup style="color:#66f1fb;">4</sup></span>
            </div> -->
            <!-- fmy -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=4jopLoEAAAAJ">Wenxuan Shen</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#ff2b67;">*</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/Episoode">Mingjia Wang</a>
                <sup style="color:#ec8bfd;">2</sup>
                <sup style="color:#ff2b67;">*</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=pcqtbQEAAAAJ&hl=en">Yaochen Wang</a
                <sup style="color:#2bff32;">2</sup>,
              </span>
              <span class="author-block">
                <a href="">Dongping Chen</a>
                <sup style="color:#fabb55;">3</sup>
                <sup style="color:#c9892e;">‡</sup>,
              </span>
              <span class="author-block">
                <a href="http://wanyao.me/">Yao Wan</a>
                <sup style="color:#2bff32;">2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ranjaykrishna.com/index.html">Weiwei Lin</a>
                <sup style="color:#ec8bfd;">1</sup>
                <sup style="color:#c9892e;">†</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup style="color:#ec8bfd;">1</sup> South China University of Technology
              </span><br>
              <span class="author-block">
                <sup style="color:#2bff32;">2</sup> Huazhong University of Science and Technology
              </span><br>
              <span class="author-block">
                <sup style="color:#fabb55;">3</sup> University of Maryland
              </span><br>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="mailto:linww@scut.edu.cn">linww@scut.edu.cn</a>
              </span>
            </div>



            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!--                <span class="link-block">-->
                <!--                  <a href="https://arxiv.org/pdf/2011.12948" class="external-link button is-normal is-rounded is-dark">-->
                <!--                    <span class="icon">-->
                <!--                      <i class="fas fa-file-pdf"></i>-->
                <!--                    </span>-->
                <!--                    <span>Paper</span>-->
                <!--                  </a>-->
                <!--                </span>-->
                <span class="link-block">
                  <!-- Arxiv link -->
                  <a href="https://arxiv.org/abs/2504.05288" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!--                <span class="link-block">-->
                <!--                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
                <!--                    class="external-link button is-normal is-rounded is-dark">-->
                <!--                    <span class="icon">-->
                <!--                      <i class="fab fa-youtube"></i>-->
                <!--                    </span>-->
                <!--                    <span>Video</span>-->
                <!--                  </a>-->
                <!--                </span>-->
                <!-- Code Link. -->

                <!-- github links -->
                <span class="link-block"> 
                  <a href="https://github.com/Episoode/Double-Bench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Episoode/Double-Bench"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>
                <!-- Model Viewer. -->
                <!-- <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-desktop"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span> -->
                <!-- Slides Link. -->
                <!-- <span class="link-block">
                  <a href="https://docs.google.com/presentation/d/1-r889Nb9n7SeZqrj-ryNqJLoMzp7aGNU2ihO8nUdEcE/edit?usp=sharing"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-google"></i>
                    </span>
                    <span>Slides</span>
                  </a>
                </span> -->
                <!-- Twitter Link. -->
                <!-- <span class="link-block">
                  <a href="https://twitter.com/TianbaoX/status/1778781521253667267" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-twitter"></i>
                    </span>
                    <span>Twitter</span>
                  </a>
                </span> -->
                <!-- Discord Link. -->
                <!-- <span class="link-block">
                  <a href="https://discord.gg/4Gnw7eTEZR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-discord"></i>
                    </span>
                    <span>Discord</span>
                  </a>
                </span> -->

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="column is-full">
        <video controls muted loop autoplay width="100%">
          <source src="static/videos/main.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Main Figure. -->
      <h2 class="title is-3"></h2>
      <div class="content has-text-justified">
        <img src="img/double-bench-case-study.png" width="100%" alt="Double-Bench Case Study" class="responsive-image">
        <!-- <img src="img/radar.jpg" width="100%" alt="GUI-world Benchmark Overview" class="responsive-image"> -->
        <md-block>
          In this work, we introduce Double-Bench, a comprehensive benchmark for evaluating document Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs). We identify critical limitations in existing evaluation approaches and propose solutions to enable more realistic and thorough assessment. Specifically, there are three-fold major contributions:
            <ol>
            <li><b>Comprehensive Problem Analysis.</b> We diagnose four major limitations in existing document RAG evaluation: incomplete scope focusing only on specific components, unrealistic prior knowledge assumptions, ambiguous or non-unique evidence labels, and poorly designed multi-hop query synthesis that fails to evaluate genuine reasoning capabilities.</li>
            <li><b>A Novel Large-Scale Benchmark.</b> We introduce Double-Bench, the first comprehensive evaluation system for multilingual and multimodal document RAG, featuring 3,276 documents (72,880 pages) and 5,168 human-validated single- and multi-hop queries across 6 languages and 4 document types. The benchmark includes exhaustively verified evidence pages, fine-grained component assessment, and dynamic update support to address data contamination.</li>
            <li><b>Extensive Evaluation and Critical Insights.</b> We conducted comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs, and 4 document RAG frameworks, revealing that the gap between text and visual embedding models is narrowing, document RAG frameworks suffer from an "over-confidence dilemma" where they provide answers without sufficient evidence, and retrieval accuracy remains the primary bottleneck rather than generation capabilities.</li>
            </ol>
        </md-block>
      </div>
      <!--/ Main Figure. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-wdith">
          <img src="img/double-bench-comparison.png" width="100%" alt="Double-Bench Comparison" class="responsive-image">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <md-block>
              Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.
            </md-block>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Double-Bench Construction</h2>
      <div class="content has-text-justified">
        <img src="img/double-bench-pipeline.png" width="100%" alt="Double-Bench Pipeline" class="responsive-image">
        <md-block>
        We introduce Double-Bench, a comprehensive multilingual and multimodal benchmark containing 3,276 documents (72,880 pages) and 5,168 human-validated queries across 6 languages and 4 document types, designed to evaluate document RAG systems holistically. The documents span high-quality PDFs, scanned documents, slides, and HTML pages collected from diverse sources including academic papers, CommonCrawl corpus, and Wikipedia entries.
        The construction of the Double-Bench dataset follows a rigorous three-stage pipeline with extensive human validation and quality control:
        <ol>
        <li><b>Metadata Collection and Preprocessing</b>: This stage involves collecting diverse document types and applying systematic filtering. Raw documents undergo coarse-grained filtering (10-50 pages, language verification using GPT-4o) followed by modality decomposition using tools like Docling and MinerU to split each page into constituent text, table, and figure components. A fine-grained content filter then reviews parsed chunks with adjacent context to ensure semantic coherence and filter out irrelevant content.</li>
        <li><b>Iterative Query Synthesis with Validation</b>: Single-hop queries are generated following four core principles (self-containment, targeting significant unimodal content, no explicit source referencing, variety and naturality) with iterative refinement using GPT-4o and validation against the corpus using high-performance embedding models until queries yield ≤5 ground truth pages. Multi-hop queries employ a knowledge graph-based approach using LightRAG, where LLM agents perform guided graph walks to generate complex reasoning chains, with each step iteratively nested to form grammatically natural questions requiring sequential reasoning.</li>
        <li><b>Post-processing and Human Refinement</b>: All generated queries undergo quality inspection against strict checklists, followed by exhaustive evidence labeling where every page is thoroughly searched and marked as evidence only if directly providing or leading to the answer. Human annotators review and adjust labels with 92% initial agreement rate, ensuring precise ground truth data and benchmark reliability.</li>
        </ol>
        </md-block>
      </div>
      </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Double-Bench Comparison</h2>
      <md-block>
        <p>📄 PDFs, 📰 Scanned documents, 🎯 Slides, 🌐 HTML pages; 
        GT: Ground Truth evidence labels, M.H.: Multi-Hop, Lang.: Supported language number, Dyna.: Support dynamic benchmark update</p>
      </div>
      </md-block>
      <div class="column is-full-width interpolation-panel">
        <div class="table-container" style="margin-bottom: 20px; overflow-x: auto;"> 
          <table class="table is-hoverable is-striped is-bordered" style="margin: 0 auto; background-color: rgba(0,0,0,0); width: 100%; text-align: center; min-width: 1200px;">
            <thead>
              <tr>
                <th style="text-align: left; position: sticky; left: 0; background-color: #f5f5f5; z-index: 10;">Benchmarks</th>
                <th colspan="2">Size</th>
                <th colspan="4">Queries</th>
                <th colspan="2">Labels</th>
                <th colspan="3">Evaluation Target</th>
                <th colspan="3">Document</th>
              </tr>
              <tr>
                <th style="text-align: left; position: sticky; left: 0; background-color: #f5f5f5; z-index: 10;"></th>
                <th>Doc</th>
                <th>Avg. #Pages</th>
                <th>Query</th>
                <th>Clarity</th>
                <th>i.i.d.</th>
                <th>M.H.</th>
                <th>GT</th>
                <th>M.H. Chain</th>
                <th>Embed Model</th>
                <th>MLLMs</th>
                <th>RAG System</th>
                <th>Lang.</th>
                <th>Dyna.</th>
                <th>Type</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #f9f9f9; z-index: 9;">DocVQA</td>
                <td>6,071</td>
                <td>1.0</td>
                <td>50,000</td>
                <td>✗</td>
                <td>✗</td>
                <td>✗</td>
                <td>✗</td>
                <td>-</td>
                <td>✗</td>
                <td>✓</td>
                <td>✗</td>
                <td>1</td>
                <td>✗</td>
                <td>📄 📰</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #f9f9f9; z-index: 9;">MMLongbench-Doc</td>
                <td>135</td>
                <td>47.5</td>
                <td>1,082</td>
                <td>✗</td>
                <td>✓</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>✗</td>
                <td>✓</td>
                <td>✗</td>
                <td>1</td>
                <td>✗</td>
                <td>📄</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #f9f9f9; z-index: 9;">MMDocIR</td>
                <td>6,818</td>
                <td>65.1</td>
                <td>73,843</td>
                <td>✗</td>
                <td>✗</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>1</td>
                <td>✗</td>
                <td>📄</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #f9f9f9; z-index: 9;">UDA-QA</td>
                <td>2,965</td>
                <td>46.3</td>
                <td>29,590</td>
                <td>✗</td>
                <td>✓</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>1</td>
                <td>✗</td>
                <td>📄</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #f9f9f9; z-index: 9;">ViDoRe v1</td>
                <td>5,000</td>
                <td>1.0</td>
                <td>500</td>
                <td>✓</td>
                <td>✓</td>
                <td>✗</td>
                <td>✓</td>
                <td>-</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>2</td>
                <td>✗</td>
                <td>📄 📰</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #f9f9f9; z-index: 9;">ViDoRe v2</td>
                <td>65</td>
                <td>48.6</td>
                <td>913</td>
                <td>✓</td>
                <td>✓</td>
                <td>✗</td>
                <td>✓</td>
                <td>-</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>2</td>
                <td>✗</td>
                <td>📄</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #f9f9f9; z-index: 9;">ViDoSeek</td>
                <td>1,142</td>
                <td>18.4</td>
                <td>1,142</td>
                <td>✓</td>
                <td>✗</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>✓</td>
                <td>✗</td>
                <td>✓</td>
                <td>1</td>
                <td>✗</td>
                <td>🎯</td>
              </tr>
              <tr>
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #f9f9f9; z-index: 9;">REAL-MM-RAG</td>
                <td>163</td>
                <td>49.1</td>
                <td>4,553</td>
                <td>✓</td>
                <td>✓</td>
                <td>✗</td>
                <td>✓</td>
                <td>-</td>
                <td>✓</td>
                <td>✗</td>
                <td>✗</td>
                <td>1</td>
                <td>✗</td>
                <td>🎯</td>
              </tr>
              <tr style="background-color: #e8f4f8;">
                <td style="font-weight: bold; text-align: left; position: sticky; left: 0; background-color: #e8f4f8; z-index: 9;">Double-Bench</td>
                <td><strong>3,276</strong></td>
                <td><strong>22.3</strong></td>
                <td><strong>5,168</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>6</strong></td>
                <td><strong>✓</strong></td>
                <td><strong>📄 📰 🎯 🌐</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
      </div>
  </section>

  <section>
    <div class="container is-max-desktop">

      <h2 class="title is-3">Benchmark Results for LIVEVQA</h2>
      <md-block>
        We conducted a comprehensive benchmark of 17 state-of-the-art Multimodal Large Language Models (MLLMs) to evaluate their capabilities in seeking and updating live visual knowledge. The evaluation was performed on the LIVEVQA dataset, which includes content from recent news articles, YouTube videos, and academic papers. Performance was measured for Level 1 (visual entity recognition) and Level 2 (deeper visual knowledge reasoning) questions, with and without various search augmentation methods.
        Key findings indicate that current MLLMs struggle significantly with visual knowledge beyond their training cutoff, but performance is drastically improved with the use of multimodal search tools.
      </md-block>
    </div>
    <div class="cover" id="contentCover">
      <div class="container-t">
        <div class="row">
          <div class="col-md-12">
            <div class="infoCard">
              <div class="infoBody">
                <div class="tabs is-centered example_lst">
                  <ul>
                    <li class="is-active"><a title="Embedding Models">Embedding Models</a></li>
                    <li><a title="Document RAG Systems">Document RAG Systems</a></li>
                    <li><a title="Multilingual Performance">Multilingual Performance</a></li>
                    <li><a title="MLLM Performance">MLLM Performance</a></li>
                  </ul>
                </div>
                <script type="text/javascript">
                  document.querySelectorAll(".example_lst li").forEach(e => {
                    e.addEventListener("click", Click_1)
                  })

                  function Click_1(eve) {
                    const iTxt = eve.target.closest('a').title; // Use title attribute for reliable selection
                    document.querySelectorAll(".example_lst li").forEach(v_li => {
                      if (iTxt === v_li.querySelector('a').title) {
                        v_li.classList.add("is-active");
                      } else {
                        v_li.classList.remove("is-active");
                      }
                    });
                    document.querySelectorAll(".lib_examples").forEach(block => {
                      block.style.display = (block.title === iTxt) ? 'block' : 'none';
                    });
                  }
                  // Initialize first tab
                  document.addEventListener('DOMContentLoaded', () => {
                    const firstTabLink = document.querySelector(".example_lst li.is-active a");
                    if (firstTabLink) {
                        Click_1({ target: firstTabLink });
                    }
                  });
                </script>

                <div title="Embedding Models" class="lib_examples" id="BoardPanel1" style="display: block;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Retrieval accuracy of state-of-the-art text and multimodal embedding models across query types, showing performance degradation as reasoning complexity increases.
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size: 0.85em;">
                      <thead>
                        <tr>
                          <th rowspan="2">Model</th>
                          <th colspan="3">Average</th>
                          <th colspan="3">Single Hop</th>
                          <th colspan="3">2-Hop</th>
                          <th colspan="3">3-Hop</th>
                        </tr>
                        <tr>
                          <th>hit@1</th>
                          <th>hit@3</th>
                          <th>hit@5</th>
                          <th>hit@1</th>
                          <th>hit@3</th>
                          <th>hit@5</th>
                          <th>hit@1</th>
                          <th>hit@3</th>
                          <th>hit@5</th>
                          <th>hit@1</th>
                          <th>hit@3</th>
                          <th>hit@5</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="13" style="font-weight:bold; text-align:center; background-color:#f0f0f0; color:#8e44ad;"><em>Text Embedding Models</em></td></tr>
                        <tr>
                          <td style="font-weight:bold;">Qwen3-Embedding-4B</td>
                          <td>0.489</td>
                          <td>0.699</td>
                          <td>0.776</td>
                          <td>0.726</td>
                          <td>0.852</td>
                          <td><strong>0.886</strong></td>
                          <td>0.314</td>
                          <td>0.598</td>
                          <td>0.663</td>
                          <td>0.235</td>
                          <td>0.531</td>
                          <td><strong>0.668</strong></td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">NV-Embed-v2</td>
                          <td>0.443</td>
                          <td>0.650</td>
                          <td>0.724</td>
                          <td>0.626</td>
                          <td>0.756</td>
                          <td>0.796</td>
                          <td><strong>0.333</strong></td>
                          <td><strong>0.604</strong></td>
                          <td><strong>0.689</strong></td>
                          <td><strong>0.240</strong></td>
                          <td>0.526</td>
                          <td>0.641</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">gte-Qwen2-7B-instruct</td>
                          <td>0.404</td>
                          <td>0.611</td>
                          <td>0.697</td>
                          <td>0.585</td>
                          <td>0.749</td>
                          <td>0.804</td>
                          <td>0.288</td>
                          <td>0.503</td>
                          <td>0.603</td>
                          <td>0.205</td>
                          <td>0.466</td>
                          <td>0.588</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">bge-m3</td>
                          <td>0.355</td>
                          <td>0.525</td>
                          <td>0.591</td>
                          <td>0.527</td>
                          <td>0.648</td>
                          <td>0.695</td>
                          <td>0.180</td>
                          <td>0.366</td>
                          <td>0.428</td>
                          <td>0.182</td>
                          <td>0.412</td>
                          <td>0.502</td>
                        </tr>
                        <tr><td colspan="13" style="font-weight:bold; text-align:center; background-color:#f0f0f0; color:#8e44ad;"><em>Visual & Multimodal Embedding Models</em></td></tr>
                        <tr>
                          <td style="font-weight:bold;">colqwen2.5-3b-multilingual</td>
                          <td><strong>0.533</strong></td>
                          <td><strong>0.727</strong></td>
                          <td><strong>0.795</strong></td>
                          <td><strong>0.778</strong></td>
                          <td><strong>0.865</strong></td>
                          <td><strong>0.895</strong></td>
                          <td>0.326</td>
                          <td><strong>0.622</strong></td>
                          <td><strong>0.693</strong></td>
                          <td><strong>0.277</strong></td>
                          <td><strong>0.579</strong></td>
                          <td><strong>0.696</strong></td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">vdr-2b-multi</td>
                          <td>0.463</td>
                          <td>0.648</td>
                          <td>0.725</td>
                          <td>0.688</td>
                          <td>0.813</td>
                          <td>0.847</td>
                          <td>0.283</td>
                          <td>0.491</td>
                          <td>0.589</td>
                          <td>0.225</td>
                          <td>0.482</td>
                          <td>0.606</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">jina-embeddings-v4</td>
                          <td>0.451</td>
                          <td>0.641</td>
                          <td>0.720</td>
                          <td>0.671</td>
                          <td>0.804</td>
                          <td>0.844</td>
                          <td>0.264</td>
                          <td>0.468</td>
                          <td>0.570</td>
                          <td>0.222</td>
                          <td>0.479</td>
                          <td>0.603</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">gme-Qwen2-VL-7B-Instruct</td>
                          <td>0.428</td>
                          <td>0.614</td>
                          <td>0.697</td>
                          <td>0.638</td>
                          <td>0.775</td>
                          <td>0.822</td>
                          <td>0.249</td>
                          <td>0.472</td>
                          <td>0.579</td>
                          <td>0.208</td>
                          <td>0.449</td>
                          <td>0.570</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">colpali-v1.3</td>
                          <td>0.403</td>
                          <td>0.571</td>
                          <td>0.646</td>
                          <td>0.584</td>
                          <td>0.679</td>
                          <td>0.717</td>
                          <td>0.230</td>
                          <td>0.440</td>
                          <td>0.525</td>
                          <td>0.220</td>
                          <td>0.469</td>
                          <td>0.588</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                </div>

                <div title="Document RAG Systems" class="lib_examples" id="BoardPanel2" style="display: none;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Performance of RAG Frameworks. Colqwen-gen achieves comparable performance with MDocAgent, the best among evaluated frameworks. This observation highlights the need for more advanced retrieval stage frameworks.
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size: 0.85em;">
                      <thead>
                        <tr>
                          <th rowspan="3">Framework</th>
                          <th colspan="4">Average</th>
                          <th colspan="4">Single Hop</th>
                          <th colspan="4">2-Hop</th>
                          <th colspan="4">3-Hop</th>
                        </tr>
                        <tr>
                          <th colspan="2">Retrieval</th>
                          <th colspan="2">Answer</th>
                          <th colspan="2">Retrieval</th>
                          <th colspan="2">Answer</th>
                          <th colspan="2">Retrieval</th>
                          <th colspan="2">Answer</th>
                          <th colspan="2">Retrieval</th>
                          <th colspan="2">Answer</th>
                        </tr>
                        <tr>
                          <th>hit@5</th>
                          <th>✓</th>
                          <th>✓</th>
                          <th>✗</th>
                          <th>hit@5</th>
                          <th>✓</th>
                          <th>✓</th>
                          <th>✗</th>
                          <th>hit@5</th>
                          <th>✓</th>
                          <th>✓</th>
                          <th>✗</th>
                          <th>hit@5</th>
                          <th>✓</th>
                          <th>✓</th>
                          <th>✗</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td style="font-weight:bold;">MDocAgent (Han et al., 2025)</td>
                          <td>0.688</td>
                          <td><strong>0.645</strong></td>
                          <td>0.126</td>
                          <td>0.229</td>
                          <td>0.830</td>
                          <td><strong>0.757</strong></td>
                          <td>0.132</td>
                          <td>0.111</td>
                          <td>0.572</td>
                          <td><strong>0.567</strong></td>
                          <td>0.065</td>
                          <td>0.367</td>
                          <td>0.549</td>
                          <td><strong>0.532</strong></td>
                          <td>0.135</td>
                          <td>0.332</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">ViDoRAG (Wang et al., 2025)</td>
                          <td>0.682</td>
                          <td>0.536</td>
                          <td>0.138</td>
                          <td>0.326</td>
                          <td>0.822</td>
                          <td>0.623</td>
                          <td>0.144</td>
                          <td>0.233</td>
                          <td>0.539</td>
                          <td>0.457</td>
                          <td>0.112</td>
                          <td>0.431</td>
                          <td>0.544</td>
                          <td>0.447</td>
                          <td>0.137</td>
                          <td>0.416</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">M3DOCRAG (Cho et al., 2024)</td>
                          <td>0.608</td>
                          <td>0.451</td>
                          <td>0.121</td>
                          <td>0.428</td>
                          <td>0.709</td>
                          <td>0.538</td>
                          <td>0.138</td>
                          <td>0.324</td>
                          <td>0.490</td>
                          <td>0.330</td>
                          <td>0.088</td>
                          <td>0.582</td>
                          <td>0.519</td>
                          <td>0.382</td>
                          <td>0.110</td>
                          <td>0.508</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">Colqwen-gen (Faysse et al., 2024)</td>
                          <td><strong>0.795</strong></td>
                          <td>0.604</td>
                          <td>0.135</td>
                          <td>0.261</td>
                          <td><strong>0.895</strong></td>
                          <td>0.676</td>
                          <td>0.160</td>
                          <td>0.164</td>
                          <td><strong>0.693</strong></td>
                          <td>0.462</td>
                          <td>0.143</td>
                          <td>0.395</td>
                          <td><strong>0.696</strong></td>
                          <td>0.554</td>
                          <td>0.100</td>
                          <td>0.346</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                </div>

                 <div title="Multilingual Performance" class="lib_examples" id="BoardPanel3" style="display: none;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Average hit@5 performance across languages. Textual embedding modelQwen3-Embedding-4B even outperform multimodal models and frameworks in low-resource languages such as Arabic and Japanese, unveiling the challenge of current multimodal embedding models’ generalizability to low-resource domains.
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size: 0.85em;">
                      <thead>
                        <tr>
                          <th>Model & Framework</th>
                          <th>Arabic</th>
                          <th>Chinese</th>
                          <th>English</th>
                          <th>French</th>
                          <th>Japanese</th>
                          <th>Spanish</th>
                          <th>Average</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="8" style="font-weight:bold; text-align:center; background-color:#f0f0f0; color:#8e44ad;"><em>Text Embedding Models</em></td></tr>
                        <tr>
                          <td style="font-weight:bold;">Qwen3-Embedding-4B</td>
                          <td><strong>0.685</strong></td>
                          <td>0.696</td>
                          <td>0.809</td>
                          <td>0.646</td>
                          <td><strong>0.801</strong></td>
                          <td>0.754</td>
                          <td>0.732</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">NV-Embed-v2</td>
                          <td>0.546</td>
                          <td>0.654</td>
                          <td><strong>0.819</strong></td>
                          <td>0.660</td>
                          <td>0.662</td>
                          <td>0.698</td>
                          <td>0.673</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">gte-Qwen2-7B-instruct</td>
                          <td><strong>0.600</strong></td>
                          <td>0.623</td>
                          <td>0.721</td>
                          <td>0.625</td>
                          <td>0.722</td>
                          <td>0.657</td>
                          <td>0.658</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">bge-m3</td>
                          <td>0.331</td>
                          <td>0.451</td>
                          <td>0.727</td>
                          <td>0.453</td>
                          <td>0.486</td>
                          <td>0.489</td>
                          <td>0.490</td>
                        </tr>
                        <tr><td colspan="8" style="font-weight:bold; text-align:center; background-color:#f0f0f0; color:#8e44ad;"><em>Visual & Multimodal Embedding Models</em></td></tr>
                        <tr>
                          <td style="font-weight:bold;">colqwen2.5-3b-multilingual</td>
                          <td>0.427</td>
                          <td>0.694</td>
                          <td><strong>0.860</strong></td>
                          <td>0.702</td>
                          <td>0.786</td>
                          <td><strong>0.798</strong></td>
                          <td><strong>0.711</strong></td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">vdr-2b-multi</td>
                          <td>0.364</td>
                          <td>0.680</td>
                          <td>0.782</td>
                          <td>0.607</td>
                          <td>0.740</td>
                          <td>0.711</td>
                          <td>0.647</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">jina-embeddings-v4</td>
                          <td>0.369</td>
                          <td>0.587</td>
                          <td>0.792</td>
                          <td>0.602</td>
                          <td>0.743</td>
                          <td>0.685</td>
                          <td>0.630</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">gme-Qwen2-VL-7B-Instruct</td>
                          <td>0.352</td>
                          <td>0.631</td>
                          <td>0.750</td>
                          <td>0.585</td>
                          <td>0.686</td>
                          <td>0.693</td>
                          <td>0.616</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">colpali-v1.3</td>
                          <td>0.271</td>
                          <td>0.300</td>
                          <td>0.781</td>
                          <td>0.607</td>
                          <td>0.335</td>
                          <td>0.694</td>
                          <td>0.498</td>
                        </tr>
                        <tr><td colspan="8" style="font-weight:bold; text-align:center; background-color:#f0f0f0; color:#8e44ad;"><em>Document RAG System</em></td></tr>
                        <tr>
                          <td style="font-weight:bold;">MDocAgent</td>
                          <td>0.457</td>
                          <td>0.679</td>
                          <td><strong>0.785</strong></td>
                          <td>0.658</td>
                          <td>0.707</td>
                          <td>0.661</td>
                          <td>0.658</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">ViDoRAG</td>
                          <td>0.455</td>
                          <td>0.676</td>
                          <td>0.778</td>
                          <td>0.654</td>
                          <td>0.702</td>
                          <td>0.668</td>
                          <td>0.655</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">M3DOCRAG</td>
                          <td>0.377</td>
                          <td>0.579</td>
                          <td>0.704</td>
                          <td>0.558</td>
                          <td>0.621</td>
                          <td>0.599</td>
                          <td>0.573</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">Colqwen-gen</td>
                          <td>0.432</td>
                          <td><strong>0.821</strong></td>
                          <td>0.793</td>
                          <td><strong>0.829</strong></td>
                          <td>0.781</td>
                          <td>0.771</td>
                          <td><strong>0.738</strong></td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                </div>

                <div title="MLLM Performance" class="lib_examples" id="BoardPanel4" style="display: none;">
                  <md-block style="text-align: center; margin-bottom: 15px;">
                    Evaluation of MLLMs’ long document understanding capability. Oracle means directly providing evidence page. The stark contrast between w.o. RAG and Oracle settings of all MLLMs reflect the high quality and low-contaminated of our Double-Bench.
                  </md-block>
                  <div class="table-container">
                    <table class="table is-striped is-hoverable is-bordered" style="width:100%; font-size: 0.85em;">
                      <thead>
                        <tr>
                          <th rowspan="2">Model & Setting</th>
                          <th colspan="3">Single Hop</th>
                          <th colspan="3">Multi Hop</th>
                        </tr>
                        <tr>
                          <th>✓</th>
                          <th>✓</th>
                          <th>✗</th>
                          <th>✓</th>
                          <th>✓</th>
                          <th>✗</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr><td colspan="7" style="font-weight:bold; text-align:center; background-color:#f0f0f0; color:#8e44ad;"><em>Models w.o. RAG</em></td></tr>
                        <tr>
                          <td style="font-weight:bold;">Qwen3-32B text-only</td>
                          <td>0.242</td>
                          <td>0.488</td>
                          <td>0.271</td>
                          <td>0.193</td>
                          <td>0.293</td>
                          <td>0.515</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">Qwen2.5-VL-7B w.o. RAG</td>
                          <td>0.053</td>
                          <td>0.557</td>
                          <td>0.390</td>
                          <td>0.127</td>
                          <td>0.168</td>
                          <td>0.705</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">GPT-4o w.o. RAG</td>
                          <td>0.109</td>
                          <td>0.748</td>
                          <td>0.144</td>
                          <td>0.197</td>
                          <td>0.332</td>
                          <td>0.472</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">Qwen2.5-VL-32B w.o. RAG</td>
                          <td>0.200</td>
                          <td>0.621</td>
                          <td>0.179</td>
                          <td>0.159</td>
                          <td>0.319</td>
                          <td>0.521</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">Llama 4 Maverick w.o. RAG</td>
                          <td><strong>0.245</strong></td>
                          <td>0.480</td>
                          <td>0.275</td>
                          <td><strong>0.215</strong></td>
                          <td>0.193</td>
                          <td>0.592</td>
                        </tr>
                        <tr><td colspan="7" style="font-weight:bold; text-align:center; background-color:#f0f0f0; color:#8e44ad;"><em>Models Oracle</em></td></tr>
                        <tr>
                          <td style="font-weight:bold;">Qwen2.5-VL-7B Oracle</td>
                          <td>0.406</td>
                          <td>0.490</td>
                          <td>0.104</td>
                          <td>0.456</td>
                          <td>0.241</td>
                          <td>0.303</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">GPT-4o Oracle</td>
                          <td>0.678</td>
                          <td>0.141</td>
                          <td>0.181</td>
                          <td>0.538</td>
                          <td>0.271</td>
                          <td>0.191</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">Llama 4 Maverick Oracle</td>
                          <td>0.601</td>
                          <td>0.350</td>
                          <td>0.049</td>
                          <td>0.524</td>
                          <td>0.192</td>
                          <td>0.284</td>
                        </tr>
                        <tr>
                          <td style="font-weight:bold;">Qwen2.5-VL-32B Oracle</td>
                          <td><strong>0.874</strong></td>
                          <td>0.061</td>
                          <td>0.066</td>
                          <td><strong>0.643</strong></td>
                          <td>0.312</td>
                          <td>0.045</td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="Empirical Result">
    <div class="container is-max-desktop">
      <div class="featurecard-container">
        <h1 class="title">Empirical Results from Double-Bench</h1>

        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Document-specified embedding model outperform general ones, and gap between text and image embedding models is narrowing.</h2>
          </div>
          <div class="description">
            <p>
              Double-Bench provides a clear divergence in the retrieval performance of various embedding models. The model rankings within Double-Bench align well with popular text embedding leaderboards MTEB and document retrieval benchmark ViDoRe v2, demonstrating the robustness of our benchmark. 
            </p>
            <p>
              ColQwen2.5-3B significantly outperforms general multimodal embedding models like jina-embeddings-v4 and GME, achieving a 9% higher average hit rate and demonstrating strong potential in document retrieval. Other multimodal embedding models show limited capability, even underperforming compared to the purely textual embedding model Qwen3-Embedding. We attribute this to recent advancements of text embedding community's sophisticated training techniques, including complex multi-stage training, dedicated hard negative sampling, and large-scale high-quality data synthesis. These techniques are difficult to transfer to visual embedding models due to training costs, limited text-and-image data, and model structural constraints. Although visual embedding models have inherent advantages for visual content retrieval, the semantic complexity of document RAG tasks negates this advantage. The critical influence of both visual observation and textual understanding abilities incentive combined strategies such as interleaved embedding models and advanced multimodal understanding pipelines.
            </p>
          </div>
        </div>

        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Double-Bench is high-quality and low contaminated that MLLMs still needs retrieval details to answer question correctly.</h2>
          </div>
          <div class="description">
            <p>
              State-of-the-art MLLMs like GPT-4o, Gemini, and Qwen are able to make general responses without context, with 50% to 70% of responses being partially correct. Providing evidence pages to MLLMs substantially boosts accuracy, with 3x to 5x responses being completely correct compared to w.o. RAG setting. This indicates that our benchmark is well-suited for evaluating the retrieval and synthesis components of RAG systems, as it clearly distinguishes context-grounded reasoning from a model's inherent knowledge. Notably, the robust performance of Qwen2.5-VL observed in the upper bound setting, which closely mirrors our benchmark curation pipeline, further suggesting the robustness and effectiveness of our pipeline in identifying correct evidence pages of queries.
            </p>
          </div>
        </div>

        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Document RAG frameworks bottleneck still lies on retrieval accuracy, where designing advanced strategies may help.</h2>
          </div>
          <div class="description">
            <p>
              Most frameworks strive to design complex information mining pipelines to extract maximum value from retrieved pages, yet tend to pay little attention to the retrieval stage itself. However, our experiments demonstrate strong correlations between retrieval accuracy and answer accuracy. Equipped with a single MLLM pass, Colqwen-gen even partially outperforms MDocAgent on multi-hop queries, despite the latter seamlessly integrating multiple agents to provide final answers. This underscores the critical importance of optimizing the retrieval stage, potentially through finer-grained document preprocessing, exploiting the hierarchical and semantic structure of documents and developing more powerful or integrated embedding models.
            </p>
          </div>
        </div>

        <div class="feature_1x1">
          <div class="featurecard">
            <h2>The overconfidence dilemma: trading trustworthiness for answers.</h2>
          </div>
          <div class="description">
            <p>
              To investigate the bottleneck in existing RAG frameworks, we breakdown each reponse of M3DocRAG and MDocAgent to analyze whether the error comes from retrieval or answering, and look into the trade-off between answering accuracy and the ability to identify insufficient information (also known as honesty). 
            </p>
            <p>
              Our experiments reveal a striking divergence in agent behavior. Simpler agents like M3DocRAG adopt a cautious strategy, answering a lower proportion of queries with successfully retrieved context but reliably identifying retrieval failures and refusing to respond. In contrast, more complex agents like MDocAgent and ViDoRAG exhibit significant overconfidence. While they achieve higher accuracy on retrieval hits, they indiscriminately attempt to answer nearly every query, regardless of whether sufficient information was retrieved. This frequently leads to speculative or entirely hallucinated content when evidence pages are missed.
            </p>
            <p>
              This observation indicates that recent document RAG development has over-emphasized maximizing answer generation at the expense of "epistemic humility", i.e., the crucial skill of knowing what it doesn't know and admitting when an answer cannot be found. Consequently, we argue that future research should pursue more trustworthy RAG frameworks where identifying informational gaps is as valued as accuracy.
            </p>
          </div>
        </div>
        <div class="feature_1x1">
          <div class="featurecard">
            <h2>Inference patterns of MLLMs as response model.</h2>
          </div>
          <div class="description">
            <p>
              We also observe different answering strategy in MLLMs. When directly provided with a multi-hop query, response model tend not to process them hop-by-hop.  On the contrary, they first collect signature information---the most distinguishing or identifiable pieces---from the various hops. Following this, models tend to perform a direct inclusion based elimination to arrive the final answer. This mechanism differentiates significantly from our expectation of how models might sequentially solve multi-hop queries. This provides a compelling point of view: merely increasing the number of hops may not increase its difficulty.
A case study in Appendix reveals that MLLMs do not process multi-hop queries step-by-step as expected. Instead, they gather key signature information from each hop and use inclusion-based elimination to find the answer. This challenges the assumption that more hops always increase difficulty, suggesting further investigation is needed.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="Acknoledgement">
    <div class="container is-max-desktop content">
      <h1 class="supportTitle">Acknowledgement</h1>
        Many thanks to members in <a href="https://oneslab.github.io">ONE Lab</a> for their invalueble effort in this project. Also thanks to video game <a href="https://store.steampowered.com/app/3527290/PEAK/">PEAK</a> for the infinite happiness and fun.
        This website is based on templates in <a href="https://livevqa.github.io">LiveVQA</a>.
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h1 class="title">BibTeX</h1>
      <pre><code>
</code></pre>
    </div>
  </section>


  <div class="content">
    <div id="supportContainer">
      <h1 class="supportTitle">Double-BenchTeam</h1>
      <br>

      <div id="logoContainer">
        <img src="img/logos/HUST.png" alt="School 1" class="schoolLogo">
        <img src="img/logos/SCUT.png" alt="School 2" class="schoolLogo">
        <img src="img/logos/UMD.png" alt="School 3" class="schoolLogo">
        <!-- <img src="img/logos/ND.png" alt="School 3" class="schoolLogo"> -->
        <!-- <img src="img/logos/uic1.png" alt="School 4" class="schoolLogo"> -->

      </div>


    </div>
  </div>
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.

            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>